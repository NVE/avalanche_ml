{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "static-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from avaml.aggregatedata import ForecastDataset, LabeledData, REG_ENG, CsvMissingError\n",
    "from utils.preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "competent-fleet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "# we need to set up gpus for tensorflow-gpu and cudnn\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "print(gpus)\n",
    "\n",
    "tf.config.experimental.set_memory_growth(device=gpus[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "personalized-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-scholar",
   "metadata": {},
   "source": [
    "# Download, read in, preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "resident-merchant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading csv\n"
     ]
    }
   ],
   "source": [
    "model_prefix = ''\n",
    "days = 7\n",
    "regobs_types = list(REG_ENG.keys())\n",
    "labeled_data = None\n",
    "try:\n",
    "    print(\"Reading csv\")\n",
    "    labeled_data = LabeledData.from_csv(days=days, regobs_types=regobs_types, with_varsom=True)\n",
    "except CsvMissingError:\n",
    "    print(\"Csv missing. Fetching online data. (This takes a long time.)\")\n",
    "    labeled_data = ForecastDataset(regobs_types=regobs_types).label(days=days, with_varsom=True)\n",
    "    labeled_data.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "composite-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data, extract data and labels\n",
    "labeled_data = preprocess(labeled_data)\n",
    "regions = labeled_data.data.reorder_levels([1, 0])\n",
    "labels = labeled_data.label.reorder_levels([1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acting-prevention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>danger_level_1</th>\n",
       "      <th>danger_level_2</th>\n",
       "      <th>danger_level_3</th>\n",
       "      <th>danger_level_4</th>\n",
       "      <th>danger_level_5</th>\n",
       "      <th>danger_level_6</th>\n",
       "      <th>emergency_warning_1</th>\n",
       "      <th>emergency_warning_2</th>\n",
       "      <th>emergency_warning_3</th>\n",
       "      <th>emergency_warning_4</th>\n",
       "      <th>...</th>\n",
       "      <th>regobs_snowprofile_t_min_4_4</th>\n",
       "      <th>regobs_snowprofile_t_min_4_5</th>\n",
       "      <th>regobs_snowprofile_t_min_4_6</th>\n",
       "      <th>regobs_snowprofile_t_min_4_7</th>\n",
       "      <th>accuracy_2</th>\n",
       "      <th>accuracy_3</th>\n",
       "      <th>accuracy_4</th>\n",
       "      <th>accuracy_5</th>\n",
       "      <th>accuracy_6</th>\n",
       "      <th>accuracy_7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3003</th>\n",
       "      <th>2017-12-07</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <th>2017-12-07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <th>2017-12-07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3010</th>\n",
       "      <th>2017-12-07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3011</th>\n",
       "      <th>2017-12-07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3031</th>\n",
       "      <th>2021-01-12</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3032</th>\n",
       "      <th>2021-01-12</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <th>2021-01-12</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3035</th>\n",
       "      <th>2021-01-12</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3037</th>\n",
       "      <th>2021-01-12</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12651 rows × 6516 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   danger_level_1  danger_level_2  danger_level_3  \\\n",
       "region date                                                         \n",
       "3003   2017-12-07             1.0             1.0             1.0   \n",
       "3007   2017-12-07             3.0             2.0             3.0   \n",
       "3009   2017-12-07             3.0             2.0             2.0   \n",
       "3010   2017-12-07             3.0             3.0             3.0   \n",
       "3011   2017-12-07             3.0             3.0             3.0   \n",
       "...                           ...             ...             ...   \n",
       "3031   2021-01-12             3.0             3.0             3.0   \n",
       "3032   2021-01-12             3.0             3.0             3.0   \n",
       "3034   2021-01-12             3.0             3.0             3.0   \n",
       "3035   2021-01-12             3.0             3.0             3.0   \n",
       "3037   2021-01-12             3.0             3.0             3.0   \n",
       "\n",
       "                   danger_level_4  danger_level_5  danger_level_6  \\\n",
       "region date                                                         \n",
       "3003   2017-12-07             1.0             1.0             1.0   \n",
       "3007   2017-12-07             2.0             2.0             2.0   \n",
       "3009   2017-12-07             2.0             2.0             2.0   \n",
       "3010   2017-12-07             2.0             2.0             2.0   \n",
       "3011   2017-12-07             2.0             2.0             2.0   \n",
       "...                           ...             ...             ...   \n",
       "3031   2021-01-12             2.0             2.0             2.0   \n",
       "3032   2021-01-12             2.0             2.0             2.0   \n",
       "3034   2021-01-12             2.0             2.0             2.0   \n",
       "3035   2021-01-12             2.0             2.0             2.0   \n",
       "3037   2021-01-12             2.0             2.0             2.0   \n",
       "\n",
       "                   emergency_warning_1  emergency_warning_2  \\\n",
       "region date                                                   \n",
       "3003   2017-12-07                  1.0                  1.0   \n",
       "3007   2017-12-07                  1.0                  1.0   \n",
       "3009   2017-12-07                  1.0                  1.0   \n",
       "3010   2017-12-07                  1.0                  1.0   \n",
       "3011   2017-12-07                  1.0                  1.0   \n",
       "...                                ...                  ...   \n",
       "3031   2021-01-12                  1.0                  1.0   \n",
       "3032   2021-01-12                  1.0                  1.0   \n",
       "3034   2021-01-12                  1.0                  1.0   \n",
       "3035   2021-01-12                  1.0                  1.0   \n",
       "3037   2021-01-12                  1.0                  1.0   \n",
       "\n",
       "                   emergency_warning_3  emergency_warning_4  ...  \\\n",
       "region date                                                  ...   \n",
       "3003   2017-12-07                  1.0                  1.0  ...   \n",
       "3007   2017-12-07                  1.0                  1.0  ...   \n",
       "3009   2017-12-07                  1.0                  1.0  ...   \n",
       "3010   2017-12-07                  1.0                  1.0  ...   \n",
       "3011   2017-12-07                  1.0                  1.0  ...   \n",
       "...                                ...                  ...  ...   \n",
       "3031   2021-01-12                  1.0                  1.0  ...   \n",
       "3032   2021-01-12                  1.0                  1.0  ...   \n",
       "3034   2021-01-12                  1.0                  1.0  ...   \n",
       "3035   2021-01-12                  1.0                  1.0  ...   \n",
       "3037   2021-01-12                  1.0                  1.0  ...   \n",
       "\n",
       "                   regobs_snowprofile_t_min_4_4  regobs_snowprofile_t_min_4_5  \\\n",
       "region date                                                                     \n",
       "3003   2017-12-07                           0.0                           0.0   \n",
       "3007   2017-12-07                           0.0                           0.0   \n",
       "3009   2017-12-07                           0.0                           0.0   \n",
       "3010   2017-12-07                           0.0                           0.0   \n",
       "3011   2017-12-07                           0.0                           0.0   \n",
       "...                                         ...                           ...   \n",
       "3031   2021-01-12                           0.0                           0.0   \n",
       "3032   2021-01-12                           0.0                           0.0   \n",
       "3034   2021-01-12                           0.0                           0.0   \n",
       "3035   2021-01-12                           0.0                           0.0   \n",
       "3037   2021-01-12                           0.0                           0.0   \n",
       "\n",
       "                   regobs_snowprofile_t_min_4_6  regobs_snowprofile_t_min_4_7  \\\n",
       "region date                                                                     \n",
       "3003   2017-12-07                           0.0                           0.0   \n",
       "3007   2017-12-07                           0.0                           0.0   \n",
       "3009   2017-12-07                           0.0                           0.0   \n",
       "3010   2017-12-07                           0.0                           0.0   \n",
       "3011   2017-12-07                           0.0                           0.0   \n",
       "...                                         ...                           ...   \n",
       "3031   2021-01-12                           0.0                           0.0   \n",
       "3032   2021-01-12                           0.0                           0.0   \n",
       "3034   2021-01-12                           0.0                           0.0   \n",
       "3035   2021-01-12                           0.0                           0.0   \n",
       "3037   2021-01-12                           0.0                           0.0   \n",
       "\n",
       "                   accuracy_2  accuracy_3  accuracy_4  accuracy_5  accuracy_6  \\\n",
       "region date                                                                     \n",
       "3003   2017-12-07         0.0         0.0         0.0         0.0         0.0   \n",
       "3007   2017-12-07         0.0         0.0         0.0         0.0         0.0   \n",
       "3009   2017-12-07         0.0         0.0         0.0         0.0         0.0   \n",
       "3010   2017-12-07         0.0         0.0         0.0         0.0         0.0   \n",
       "3011   2017-12-07         0.0         0.0         0.0         0.0         0.0   \n",
       "...                       ...         ...         ...         ...         ...   \n",
       "3031   2021-01-12         0.0         0.0         0.0         0.0         0.0   \n",
       "3032   2021-01-12         0.0         0.0         0.0         0.0         0.0   \n",
       "3034   2021-01-12         0.0         0.0         0.0         0.0         0.0   \n",
       "3035   2021-01-12         0.0         0.0         0.0         0.0         0.0   \n",
       "3037   2021-01-12         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "                   accuracy_7  \n",
       "region date                    \n",
       "3003   2017-12-07         0.0  \n",
       "3007   2017-12-07         0.0  \n",
       "3009   2017-12-07         0.0  \n",
       "3010   2017-12-07         0.0  \n",
       "3011   2017-12-07         0.0  \n",
       "...                       ...  \n",
       "3031   2021-01-12         0.0  \n",
       "3032   2021-01-12         0.0  \n",
       "3034   2021-01-12         0.0  \n",
       "3035   2021-01-12         0.0  \n",
       "3037   2021-01-12         0.0  \n",
       "\n",
       "[12651 rows x 6516 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-distinction",
   "metadata": {},
   "source": [
    "One of the first things we need to take care of is the precipitation column. There are some extremely high values throughout the dataset that we need to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "artistic-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_cols = [col for col in regions.columns if 'precip' in col and 'exposed' not in col]\n",
    "regions_precip = regions.loc[:, precip_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-advocacy",
   "metadata": {},
   "source": [
    "There is a set of anomalously high precipitation values in this dataset that we need to be mindful of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lyric-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_outliers = np.where(regions_precip > 100)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aging-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idx = regions.index[precip_outliers]\n",
    "regions = regions.drop(drop_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "entire-sucking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precip_0    95.6\n",
       "precip_1    95.6\n",
       "precip_2    95.6\n",
       "precip_3    95.6\n",
       "precip_4    95.6\n",
       "precip_5    95.6\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions.loc[:, precip_cols].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-litigation",
   "metadata": {},
   "source": [
    "These seem like more reasonable values. We need to do the same thing for the labels, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mechanical-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.drop(drop_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "orange-surrey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input data:  (12519, 6516)\n",
      "Shape of labels    :  (12519, 69)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of input data:  {}'.format(regions.shape))\n",
    "print('Shape of labels    :  {}'.format(labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "commercial-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode cause and aspect columns in labels\n",
    "labels = encode_causes(labels)\n",
    "labels = encode_aspects(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-puppy",
   "metadata": {},
   "source": [
    "In a previous Notebook analysis on the importance of different features, we say that different meteorological variables from RegObs are often more important than other fields included in the data. And what's more, combining variables to create custom features is even *more* important. We can create those features again below.\n",
    "\n",
    "**Why do we want to do feed in weather data to a neural network when other data is available?** This is a good question. Generally, we cannot send people everywhere to inspect avalanche conditions. That means that there are many places where the *only* data that we have available comes in the form of weather forecasts and observations. Thus, it is important to see if a model can predict danger level or the first avalanche problem given only weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "subsequent-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wind_speed_max(df, col_list):\n",
    "    \"\"\"\n",
    "    Create wind_speed_max_# columns from wind_speed_# and wind_change_speed_#\n",
    "    input columns. This will drop the wind_speed_# and wind_change_speed_# columns.\n",
    "    \n",
    "    Arguments:\n",
    "        df(DataFrame): the input dataframe containing our RegObs observations\n",
    "        col_list(list): a list of columns for the wind_speed_# and wind_change_speed_# features\n",
    "    \n",
    "    Returns:\n",
    "        df(DataFrame): same input data frame with new wind_speed_max_# columns\n",
    "    \"\"\"\n",
    "    # so there is a wind_speed and wind_change_speed column for each day\n",
    "    # that means that there are len(col_list) / 2 unique days\n",
    "    for day in range(int(len(col_list) / 2)):\n",
    "        speed_col = 'wind_speed_' + str(day)\n",
    "        change_col = 'wind_change_speed_' + str(day)\n",
    "        max_col = 'wind_speed_max_' +str(day)\n",
    "        \n",
    "        # take the max over rows\n",
    "        df[max_col] = df.loc[:, [speed_col, change_col]].max(axis=1)\n",
    "        \n",
    "    # drop unwanted wind columns\n",
    "    #df.drop(col_list, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "curious-resistance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_over_features(df, col_list, new_name, drop=False):\n",
    "    \"\"\"\n",
    "    Create column for maximum wind speed over the past 72 hours.\n",
    "    \n",
    "    Arguments:\n",
    "        df(DataFrame): the input dataframe containing our RegObs observations\n",
    "        col_list(list): a list of columns for a set of features; this could be wind, precip, etc.\n",
    "        new_name(str): what you want the new column name to be\n",
    "        drop(bool): whether to drop the columns in col_list before returning df\n",
    "        \n",
    "    NOTE: You need to make sure you pass in intelligible columns. For instance, it may not make sense to\n",
    "          sum over certain columns. Additionally, all of the input columns should follow the same\n",
    "          naming convection, i.e., `precip_0`, `precip_`, `precip_3`\n",
    "    \n",
    "    Returns:\n",
    "        df(DataFrame): same input dataframe with summed column, after dropping input columns\n",
    "    \"\"\"\n",
    "    # sum over rows, or each day\n",
    "    df[new_name] = df.loc[:, col_list].sum(axis=1)\n",
    "    \n",
    "    # drop unwanted columns\n",
    "    if drop == True:\n",
    "        df.drop(col_list, axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "confirmed-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_max(df, col_list, new_name, drop=False):\n",
    "    \"\"\"\n",
    "    Create a column for the max value given over the past 3 days (72 hours).\n",
    "    This will drop the input columns in col_list if drop is set to True.\n",
    "    \n",
    "    Arguments:\n",
    "        df(DataFrame): the input dataframe containing our RegObs observations\n",
    "        col_list(list): a list of columns for creating the max value\n",
    "        new_name(str): what you want the new column name to be\n",
    "        drop(bool): whether to drop the columns in col_list before returning df\n",
    "    \n",
    "    Returns:\n",
    "        df(DataFrame): same input data frame with new max value column\n",
    "    \"\"\"\n",
    "    # take the max over rows\n",
    "    df[new_name] = df.loc[:, col_list].max(axis=1)\n",
    "    \n",
    "    # drop unwanted columns\n",
    "    if drop == True:\n",
    "        df.drop(col_list, axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "intended-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop wind speed columns\n",
    "wind_dir_cols = [col for col in regions.columns if 'wind_dir' in col]\n",
    "#regions.drop(wind_dir_cols, axis=1, inplace=True)\n",
    "\n",
    "# create 72 hour sum or max of wind speed, precip, and max temp\n",
    "wind_speed_cols = [col for col in regions.columns if 'wind' in col and 'speed' in col]\n",
    "regions = wind_speed_max(regions, wind_speed_cols)\n",
    "to_sum = ['wind_speed_max_0', 'wind_speed_max_1', 'wind_speed_max_2']\n",
    "regions = sum_over_features(regions, to_sum, 'wind_speed_72hr')\n",
    "\n",
    "to_sum = ['precip_0', 'precip_1', 'precip_2']\n",
    "regions = sum_over_features(regions, to_sum, 'precip_72hr')\n",
    "\n",
    "to_sum = ['temp_max_0', 'temp_max_1', 'temp_max_2']\n",
    "regions = take_max(regions, to_sum, 'temp_max_72hr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "burning-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_include_list = ['acc', 'dir', 'fl_start', 'regobs', 'start']\n",
    "all_wx_cols = [col for col in regions.columns]\n",
    "\n",
    "for drop_col in dont_include_list:\n",
    "    all_wx_cols = [col for col in all_wx_cols if drop_col not in col]\n",
    "\n",
    "#all_wx_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "nutritional-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = regions.loc[:, all_wx_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "least-cooler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region  date      \n",
       "3003    2017-12-07    1.0\n",
       "3007    2017-12-07    3.0\n",
       "3009    2017-12-07    3.0\n",
       "3010    2017-12-07    3.0\n",
       "3011    2017-12-07    3.0\n",
       "                     ... \n",
       "3031    2021-01-12    3.0\n",
       "3032    2021-01-12    3.0\n",
       "3034    2021-01-12    3.0\n",
       "3035    2021-01-12    3.0\n",
       "3037    2021-01-12    3.0\n",
       "Name: danger_level_1, Length: 12519, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[:, 'danger_level_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "choice-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "danger_cols = [col for col in train.columns if 'danger' in col]\n",
    "\n",
    "for col in danger_cols:\n",
    "    train.loc[:, col][train.loc[:, col].values == 1] = 0\n",
    "    train.loc[:, col][train.loc[:, col].values == 2] = 0\n",
    "    train.loc[:, col][train.loc[:, col].values == 3] = 0\n",
    "    train.loc[:, col][train.loc[:, col].values == 4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "romantic-auditor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train[danger_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "liked-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_input_data(X):\n",
    "    \"\"\"\n",
    "    Scale input features from [-1, 1] so that it is easier to\n",
    "    train a neural network.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(X)\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "major-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X = scale_input_data(train.values)\n",
    "train = pd.DataFrame(scaled_X, columns=train.columns, index=train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dominant-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# could use CLASS_danger_level or CLASS_problem_1\n",
    "train_labels = labels.loc[:, ['CLASS_danger_level']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "martial-stationery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLASS_danger_level\n",
       "1                     18.82\n",
       "2                     52.81\n",
       "3                     26.48\n",
       "4                      1.89\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_occurrences = (train_labels.value_counts() / len(train_labels) * 100).sort_index()\n",
    "round(class_occurrences, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-observation",
   "metadata": {},
   "source": [
    "So we see that there is a heavy imbalance in the class weights that we are trying to predict. This is not ideal for our case, because more than anything we need to reliably be able to predict danger levels 3 and 4. Scikit-learn makes this easy to do for imbalanced classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "balanced-guard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.hist(train_labels, align='mid', )#rwidth=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-potato",
   "metadata": {},
   "source": [
    "### Outlier Detection\n",
    "\n",
    "Since the classes are so imbalanced, we could treat this like an outlier detection problem and only try to detect avalanche danger level 4's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "lucky-broadway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "train_labels[train_labels.values == 1] = 0\n",
    "train_labels[train_labels.values == 2] = 0\n",
    "train_labels[train_labels.values == 3] = 0\n",
    "train_labels[train_labels.values == 4] = 1\n",
    "\n",
    "print(np.unique(train_labels.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "charming-millennium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.hist(train_labels.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "formed-species",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLASS_danger_level\n",
       "0                     98.11\n",
       "1                      1.89\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_occurrences = (train_labels.value_counts() / len(train_labels) * 100).sort_index()\n",
    "round(class_occurrences, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-enough",
   "metadata": {},
   "source": [
    "We can set some class weights below so that the model we train prioritizes getting danger level 4's correct. Note: this can act as another form of regularization for the model.\n",
    "\n",
    "The weights are set to be about equal to what you would get with the [`compute_class_weight()`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html) method in Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "historic-complaint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5, 1: 23}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = {\n",
    "    0: 0.5,\n",
    "    1: 23\n",
    "}\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-young",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "imperial-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = [3007, 3012, 3010, 3009, 3013, 3017, 3014, 3032, \n",
    "             3027, 3029, 3022, 3031, 3023, 3037, 3024, 3028]\n",
    "val_idx =   [3011, 3016, 3035]\n",
    "test_idx =  [3006, 3015, 3034]\n",
    "\n",
    "X_train = [train.loc[idx].values for idx in train_idx]\n",
    "y_train = [train_labels.loc[idx].values for idx in train_idx]\n",
    "\n",
    "X_val = [train.loc[idx].values for idx in val_idx]\n",
    "y_val = [train_labels.loc[idx].values for idx in val_idx]\n",
    "\n",
    "X_test = [train.loc[idx].values for idx in test_idx]\n",
    "y_test = [train_labels.loc[idx].values for idx in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "stone-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(arr, length):\n",
    "    \"\"\"\n",
    "    This method will pad a m x n array so that m is perfectly\n",
    "    divisible by length. That is, m % length == 0.\n",
    "    \n",
    "    Arguments:\n",
    "        arr(array): m x n array where there are n input features of length m\n",
    "        length(int): what we want the length of of arr to be divisible by\n",
    "        \n",
    "    Returns:\n",
    "        padded(array): new padded array\n",
    "    \"\"\"\n",
    "    n_features = arr.shape[1]\n",
    "    remainder = arr.shape[0] % length\n",
    "    if remainder == 0: # then nothing to pad\n",
    "        return arr\n",
    "    \n",
    "    pad_length = length - remainder\n",
    "    to_add = np.zeros((pad_length, n_features))\n",
    "    padded = np.concatenate([arr, to_add])\n",
    "    #padded = np.concatenate([to_add, arr])\n",
    "    \n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "located-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 7 # this is an arbitrary choice, a hyperparameter\n",
    "X_train_pad = [pad_sequence(arr, sequence_length) for arr in X_train]\n",
    "y_train_pad = [pad_sequence(arr, sequence_length) for arr in y_train]\n",
    "\n",
    "X_val_pad = [pad_sequence(arr, sequence_length) for arr in X_val]\n",
    "y_val_pad = [pad_sequence(arr, sequence_length) for arr in y_val]\n",
    "\n",
    "X_test_pad = [pad_sequence(arr, sequence_length) for arr in X_test]\n",
    "y_test_pad = [pad_sequence(arr, sequence_length) for arr in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "close-assignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8933, 1335)\n",
      "(8933, 1)\n",
      "(1722, 1335)\n",
      "(1722, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape data into (samples, timesteps, features)\n",
    "X_concat_train = np.concatenate(X_train)\n",
    "y_concat_train = np.concatenate(y_train)\n",
    "\n",
    "X_concat_val = np.concatenate(X_val)\n",
    "y_concat_val = np.concatenate(y_val)\n",
    "\n",
    "print(X_concat_train.shape)\n",
    "print(y_concat_train.shape)\n",
    "\n",
    "print(X_concat_val.shape)\n",
    "print(y_concat_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-malta",
   "metadata": {},
   "source": [
    "In order to use the recall metric, you need to have your vector of binary values one-hot encoded as a 2D matrix. We can do that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "handed-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "y_concat_train = encoder.fit_transform(y_concat_train).toarray()\n",
    "y_concat_val = encoder.fit_transform(y_concat_val).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "needed-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(x, y, length):\n",
    "    \"\"\"\n",
    "    Batch the neural network data, creating a shifting window of data at each time step.\n",
    "    \"\"\"    \n",
    "    #create empty lists to append to\n",
    "    X, Y = [], []\n",
    "    \n",
    "    #iterate over dataset, looking at moving window of 1 timestep\n",
    "    #need to length to prevent out of bounds errors\n",
    "    for i in range(0, len(x)-length):\n",
    "        x_batch = x[i:length+i]\n",
    "        y_batch = y[length+i]\n",
    "        \n",
    "        #append batches to lists\n",
    "        X.append(x_batch)\n",
    "        Y.append(y_batch)\n",
    "    \n",
    "    #convert lists to numpy arrays before turning into torch tensors\n",
    "    X = np.asarray(X)\n",
    "    Y = np.asarray(Y)\n",
    "    \n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "professional-indonesia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8926, 7, 1335) (8926, 2)\n",
      "(1715, 7, 1335) (1715, 2)\n"
     ]
    }
   ],
   "source": [
    "batchX_train, batchY_train = batch_data(X_concat_train, y_concat_train, sequence_length)\n",
    "batchX_val, batchY_val = batch_data(X_concat_val, y_concat_val, sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-store",
   "metadata": {},
   "source": [
    "# Training a neural network\n",
    "\n",
    "## DNN\n",
    "We can define our neural network below. This is a multiclass classification problem, meaning that we need as many output units in the last layer as there are unique labels in our training set. It also means that we should use a softmax activation in the last layer and some form of categorical crossentropy as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "corporate-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn(X, n_input, dropout, n_output, add_reg=False):\n",
    "    \"\"\"\n",
    "    Create a DNN with or without regularization. Note: X should have\n",
    "    shape (samples, timesteps, features)\n",
    "    \"\"\"\n",
    "    timesteps = X.shape[1]\n",
    "    features = X.shape[2]\n",
    "    \n",
    "    # design network\n",
    "    dnn = tf.keras.models.Sequential()\n",
    "    if(add_reg == True):\n",
    "        reg = tf.keras.regularizers.l2(l=0.0001)\n",
    "        dnn.add(Dense(n_input, activation='elu', kernel_regularizer=reg, input_shape=(timesteps, features)))\n",
    "        dnn.add(Dropout(dropout))\n",
    "        \n",
    "        dnn.add(Dense(n_input, activation='elu', kernel_regularizer=reg))\n",
    "        dnn.add(Dropout(dropout))\n",
    "        \n",
    "        dnn.add(Dense(n_input, activation='elu', kernel_regularizer=reg))\n",
    "        \n",
    "    else:\n",
    "        dnn.add(Dense(n_input, activation='elu', input_shape=(timesteps, features)))\n",
    "        dnn.add(Dropout(dropout))\n",
    "        \n",
    "        dnn.add(Dense(n_input, activation='elu'))\n",
    "        dnn.add(Dropout(dropout))\n",
    "        \n",
    "        dnn.add(Dense(n_input, activation='elu'))\n",
    "    \n",
    "    dnn.add(Dropout(dropout))\n",
    "    dnn.add(tf.keras.layers.Flatten())\n",
    "    dnn.add(Dense(n_output, activation='sigmoid'))\n",
    "    \n",
    "    return dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "intense-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, y, X_val, y_val, loss, opt, batch, e, w):\n",
    "    \"\"\"\n",
    "    Train a neural network with passed in hyperparameters.\n",
    "    \"\"\"\n",
    "    # compile and fit model\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=opt, \n",
    "                  metrics=['Recall'])\n",
    "\n",
    "    history = model.fit(X, y, validation_data=(X_val, y_val),\n",
    "                        batch_size=batch, epochs=e, verbose=1, \n",
    "                        shuffle=False, class_weight=w)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "divine-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"\n",
    "    Plot the loss function vs epochs and metric vs epochs after\n",
    "    training a neural network.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "    # plot loss over time\n",
    "    ax[0].plot(history.history['loss'], label='train')\n",
    "    ax[0].plot(history.history['val_loss'], label='val')\n",
    "    \n",
    "    # plot metric over time\n",
    "    ax[1].plot(history.history['recall'], label='train')\n",
    "    ax[1].plot(history.history['val_recall'], label='val')\n",
    "    \n",
    "    ax[0].set_title('Loss')\n",
    "    ax[1].set_title('recall')\n",
    "    \n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "prescribed-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, X, y_true):\n",
    "    y_pred = np.argmax(model.predict(X), axis=1)\n",
    "    precision, recall, fscore, support = score(y_true, y_pred)\n",
    "    print('precision: {}'.format(precision))\n",
    "    print('recall:    {}'.format(recall))\n",
    "    print('fscore:    {}'.format(fscore))\n",
    "\n",
    "    #use seaborn's sns.heatmap() function for pretty plotting of confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    ax = sns.heatmap(cm, annot=True, fmt='d', cbar=False)\n",
    "\n",
    "    #set x and y labels, as well as title\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('ROC Confusion Matrix')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "environmental-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network params\n",
    "input_neurons = 100\n",
    "output_neurons = len(np.unique(y_concat_train))\n",
    "drop = 0.3\n",
    "\n",
    "# training params\n",
    "num_epochs = 600\n",
    "alpha = 3e-5\n",
    "batch_size = 128\n",
    "criterion = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(lr=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "directed-chest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "WARNING:tensorflow:From /home/jakidxav/anaconda3/envs/avalanche_ml/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.6746 - recall: 0.3551 - val_loss: 0.6985 - val_recall: 0.6041\n",
      "Epoch 2/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6364 - recall: 0.6254 - val_loss: 0.6297 - val_recall: 0.7294\n",
      "Epoch 3/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6296 - recall: 0.6738 - val_loss: 0.6059 - val_recall: 0.7633\n",
      "Epoch 4/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6081 - recall: 0.6984 - val_loss: 0.5971 - val_recall: 0.7405\n",
      "Epoch 5/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.6002 - recall: 0.6870 - val_loss: 0.5912 - val_recall: 0.7347\n",
      "Epoch 6/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5879 - recall: 0.6985 - val_loss: 0.5776 - val_recall: 0.7504\n",
      "Epoch 7/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5739 - recall: 0.6847 - val_loss: 0.5747 - val_recall: 0.7364\n",
      "Epoch 8/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5615 - recall: 0.7106 - val_loss: 0.5683 - val_recall: 0.7446\n",
      "Epoch 9/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5603 - recall: 0.7104 - val_loss: 0.5638 - val_recall: 0.7417\n",
      "Epoch 10/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5520 - recall: 0.7227 - val_loss: 0.5522 - val_recall: 0.7487\n",
      "Epoch 11/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5334 - recall: 0.7352 - val_loss: 0.5462 - val_recall: 0.7446\n",
      "Epoch 12/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5294 - recall: 0.7283 - val_loss: 0.5440 - val_recall: 0.7440\n",
      "Epoch 13/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5231 - recall: 0.7420 - val_loss: 0.5337 - val_recall: 0.7574\n",
      "Epoch 14/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5059 - recall: 0.7567 - val_loss: 0.5087 - val_recall: 0.7773\n",
      "Epoch 15/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5037 - recall: 0.7676 - val_loss: 0.5239 - val_recall: 0.7563\n",
      "Epoch 16/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.5049 - recall: 0.7589 - val_loss: 0.5221 - val_recall: 0.7673\n",
      "Epoch 17/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4933 - recall: 0.7690 - val_loss: 0.5124 - val_recall: 0.7732\n",
      "Epoch 18/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4858 - recall: 0.7800 - val_loss: 0.5073 - val_recall: 0.7697\n",
      "Epoch 19/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4798 - recall: 0.7825 - val_loss: 0.4996 - val_recall: 0.7720\n",
      "Epoch 20/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4625 - recall: 0.7906 - val_loss: 0.4836 - val_recall: 0.7889\n",
      "Epoch 21/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4614 - recall: 0.7979 - val_loss: 0.4829 - val_recall: 0.7802\n",
      "Epoch 22/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4639 - recall: 0.7878 - val_loss: 0.4839 - val_recall: 0.7802\n",
      "Epoch 23/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4587 - recall: 0.7965 - val_loss: 0.4719 - val_recall: 0.7872\n",
      "Epoch 24/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4416 - recall: 0.8048 - val_loss: 0.4642 - val_recall: 0.7953\n",
      "Epoch 25/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4400 - recall: 0.8107 - val_loss: 0.4638 - val_recall: 0.7983\n",
      "Epoch 26/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4311 - recall: 0.8143 - val_loss: 0.4577 - val_recall: 0.8064\n",
      "Epoch 27/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4366 - recall: 0.8199 - val_loss: 0.4586 - val_recall: 0.8017\n",
      "Epoch 28/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4308 - recall: 0.8165 - val_loss: 0.4549 - val_recall: 0.8029\n",
      "Epoch 29/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4189 - recall: 0.8220 - val_loss: 0.4496 - val_recall: 0.8076\n",
      "Epoch 30/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4148 - recall: 0.8233 - val_loss: 0.4409 - val_recall: 0.8128\n",
      "Epoch 31/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4055 - recall: 0.8274 - val_loss: 0.4241 - val_recall: 0.8262\n",
      "Epoch 32/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4102 - recall: 0.8315 - val_loss: 0.4507 - val_recall: 0.8064\n",
      "Epoch 33/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.4024 - recall: 0.8332 - val_loss: 0.4269 - val_recall: 0.8222\n",
      "Epoch 34/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3997 - recall: 0.8298 - val_loss: 0.4202 - val_recall: 0.8239\n",
      "Epoch 35/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3979 - recall: 0.8373 - val_loss: 0.4236 - val_recall: 0.8233\n",
      "Epoch 36/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3901 - recall: 0.8330 - val_loss: 0.4208 - val_recall: 0.8245\n",
      "Epoch 37/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3823 - recall: 0.8386 - val_loss: 0.4159 - val_recall: 0.8280\n",
      "Epoch 38/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.3784 - recall: 0.8426 - val_loss: 0.4057 - val_recall: 0.8344\n",
      "Epoch 39/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3817 - recall: 0.8443 - val_loss: 0.4137 - val_recall: 0.8292\n",
      "Epoch 40/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3784 - recall: 0.8476 - val_loss: 0.4053 - val_recall: 0.8332\n",
      "Epoch 41/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3791 - recall: 0.8451 - val_loss: 0.4072 - val_recall: 0.8297\n",
      "Epoch 42/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3697 - recall: 0.8463 - val_loss: 0.3967 - val_recall: 0.8402\n",
      "Epoch 43/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3660 - recall: 0.8513 - val_loss: 0.3856 - val_recall: 0.8437\n",
      "Epoch 44/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3573 - recall: 0.8590 - val_loss: 0.3763 - val_recall: 0.8507\n",
      "Epoch 45/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3615 - recall: 0.8575 - val_loss: 0.3790 - val_recall: 0.8466\n",
      "Epoch 46/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3528 - recall: 0.8582 - val_loss: 0.3904 - val_recall: 0.8385\n",
      "Epoch 47/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3563 - recall: 0.8517 - val_loss: 0.3842 - val_recall: 0.8402\n",
      "Epoch 48/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3428 - recall: 0.8637 - val_loss: 0.3789 - val_recall: 0.8420\n",
      "Epoch 49/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3399 - recall: 0.8581 - val_loss: 0.3716 - val_recall: 0.8461\n",
      "Epoch 50/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3457 - recall: 0.8619 - val_loss: 0.3759 - val_recall: 0.8420\n",
      "Epoch 51/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3503 - recall: 0.8556 - val_loss: 0.3849 - val_recall: 0.8397\n",
      "Epoch 52/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3323 - recall: 0.8609 - val_loss: 0.3685 - val_recall: 0.8496\n",
      "Epoch 53/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3370 - recall: 0.8656 - val_loss: 0.3718 - val_recall: 0.8496\n",
      "Epoch 54/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3346 - recall: 0.8619 - val_loss: 0.3673 - val_recall: 0.8519\n",
      "Epoch 55/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3292 - recall: 0.8666 - val_loss: 0.3524 - val_recall: 0.8612\n",
      "Epoch 56/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3288 - recall: 0.8677 - val_loss: 0.3660 - val_recall: 0.8513\n",
      "Epoch 57/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3273 - recall: 0.8683 - val_loss: 0.3459 - val_recall: 0.8659\n",
      "Epoch 58/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3318 - recall: 0.8668 - val_loss: 0.3704 - val_recall: 0.8507\n",
      "Epoch 59/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3163 - recall: 0.8677 - val_loss: 0.3427 - val_recall: 0.8676\n",
      "Epoch 60/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3128 - recall: 0.8765 - val_loss: 0.3540 - val_recall: 0.8636\n",
      "Epoch 61/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3184 - recall: 0.8716 - val_loss: 0.3493 - val_recall: 0.8676\n",
      "Epoch 62/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3123 - recall: 0.8755 - val_loss: 0.3584 - val_recall: 0.8595\n",
      "Epoch 63/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3047 - recall: 0.8769 - val_loss: 0.3499 - val_recall: 0.8653\n",
      "Epoch 64/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3067 - recall: 0.8810 - val_loss: 0.3472 - val_recall: 0.8653\n",
      "Epoch 65/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3038 - recall: 0.8806 - val_loss: 0.3379 - val_recall: 0.8706\n",
      "Epoch 66/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.3043 - recall: 0.8825 - val_loss: 0.3545 - val_recall: 0.8612\n",
      "Epoch 67/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2971 - recall: 0.8769 - val_loss: 0.3345 - val_recall: 0.8723\n",
      "Epoch 68/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2983 - recall: 0.8847 - val_loss: 0.3406 - val_recall: 0.8729\n",
      "Epoch 69/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2915 - recall: 0.8851 - val_loss: 0.3329 - val_recall: 0.8758\n",
      "Epoch 70/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2942 - recall: 0.8797 - val_loss: 0.3322 - val_recall: 0.8741\n",
      "Epoch 71/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2981 - recall: 0.8787 - val_loss: 0.3481 - val_recall: 0.8589\n",
      "Epoch 72/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2839 - recall: 0.8844 - val_loss: 0.3199 - val_recall: 0.8799\n",
      "Epoch 73/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2924 - recall: 0.8768 - val_loss: 0.3433 - val_recall: 0.8618\n",
      "Epoch 74/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2841 - recall: 0.8856 - val_loss: 0.3377 - val_recall: 0.8694\n",
      "Epoch 75/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2914 - recall: 0.8852 - val_loss: 0.3483 - val_recall: 0.8624\n",
      "Epoch 76/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2777 - recall: 0.8857 - val_loss: 0.3126 - val_recall: 0.8840\n",
      "Epoch 77/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2861 - recall: 0.8842 - val_loss: 0.3356 - val_recall: 0.8723\n",
      "Epoch 78/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2855 - recall: 0.8842 - val_loss: 0.3354 - val_recall: 0.8694\n",
      "Epoch 79/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2816 - recall: 0.8858 - val_loss: 0.3349 - val_recall: 0.8700\n",
      "Epoch 80/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2762 - recall: 0.8912 - val_loss: 0.3276 - val_recall: 0.8706\n",
      "Epoch 81/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2788 - recall: 0.8873 - val_loss: 0.3383 - val_recall: 0.8700\n",
      "Epoch 82/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2683 - recall: 0.8903 - val_loss: 0.3113 - val_recall: 0.8810\n",
      "Epoch 83/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2881 - recall: 0.8816 - val_loss: 0.3224 - val_recall: 0.8770\n",
      "Epoch 84/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2768 - recall: 0.8899 - val_loss: 0.3222 - val_recall: 0.8776\n",
      "Epoch 85/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2600 - recall: 0.9022 - val_loss: 0.3216 - val_recall: 0.8758\n",
      "Epoch 86/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2699 - recall: 0.8938 - val_loss: 0.3162 - val_recall: 0.8793\n",
      "Epoch 87/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2590 - recall: 0.8949 - val_loss: 0.3005 - val_recall: 0.8869\n",
      "Epoch 88/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2599 - recall: 0.8952 - val_loss: 0.3107 - val_recall: 0.8805\n",
      "Epoch 89/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2608 - recall: 0.8970 - val_loss: 0.3262 - val_recall: 0.8746\n",
      "Epoch 90/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2612 - recall: 0.8955 - val_loss: 0.3041 - val_recall: 0.8886\n",
      "Epoch 91/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2589 - recall: 0.8977 - val_loss: 0.3075 - val_recall: 0.8869\n",
      "Epoch 92/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2579 - recall: 0.8978 - val_loss: 0.3087 - val_recall: 0.8845\n",
      "Epoch 93/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2563 - recall: 0.8997 - val_loss: 0.2978 - val_recall: 0.8886\n",
      "Epoch 94/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2464 - recall: 0.9031 - val_loss: 0.2861 - val_recall: 0.8956\n",
      "Epoch 95/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2536 - recall: 0.9012 - val_loss: 0.2927 - val_recall: 0.8910\n",
      "Epoch 96/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2482 - recall: 0.9059 - val_loss: 0.3116 - val_recall: 0.8845\n",
      "Epoch 97/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2464 - recall: 0.9041 - val_loss: 0.2913 - val_recall: 0.8945\n",
      "Epoch 98/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2457 - recall: 0.9017 - val_loss: 0.2955 - val_recall: 0.8927\n",
      "Epoch 99/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2466 - recall: 0.9054 - val_loss: 0.3092 - val_recall: 0.8886\n",
      "Epoch 100/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2424 - recall: 0.9013 - val_loss: 0.2891 - val_recall: 0.8968\n",
      "Epoch 101/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2417 - recall: 0.9088 - val_loss: 0.3161 - val_recall: 0.8851\n",
      "Epoch 102/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2319 - recall: 0.9073 - val_loss: 0.2712 - val_recall: 0.9009\n",
      "Epoch 103/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2429 - recall: 0.9086 - val_loss: 0.2790 - val_recall: 0.8980\n",
      "Epoch 104/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2302 - recall: 0.9103 - val_loss: 0.2885 - val_recall: 0.8945\n",
      "Epoch 105/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2372 - recall: 0.9095 - val_loss: 0.2855 - val_recall: 0.8956\n",
      "Epoch 106/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2373 - recall: 0.9049 - val_loss: 0.3151 - val_recall: 0.8822\n",
      "Epoch 107/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2264 - recall: 0.9063 - val_loss: 0.2631 - val_recall: 0.9032\n",
      "Epoch 108/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2293 - recall: 0.9124 - val_loss: 0.2943 - val_recall: 0.8927\n",
      "Epoch 109/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2410 - recall: 0.9026 - val_loss: 0.2990 - val_recall: 0.8880\n",
      "Epoch 110/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2322 - recall: 0.9097 - val_loss: 0.2987 - val_recall: 0.8898\n",
      "Epoch 111/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2269 - recall: 0.9080 - val_loss: 0.2689 - val_recall: 0.8997\n",
      "Epoch 112/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2255 - recall: 0.9133 - val_loss: 0.2795 - val_recall: 0.8962\n",
      "Epoch 113/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2280 - recall: 0.9082 - val_loss: 0.2738 - val_recall: 0.8997\n",
      "Epoch 114/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2228 - recall: 0.9160 - val_loss: 0.2836 - val_recall: 0.8956\n",
      "Epoch 115/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2216 - recall: 0.9124 - val_loss: 0.2858 - val_recall: 0.8950\n",
      "Epoch 116/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2224 - recall: 0.9134 - val_loss: 0.2803 - val_recall: 0.8968\n",
      "Epoch 117/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2171 - recall: 0.9168 - val_loss: 0.2777 - val_recall: 0.9015\n",
      "Epoch 118/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2133 - recall: 0.9155 - val_loss: 0.2651 - val_recall: 0.9026\n",
      "Epoch 119/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2342 - recall: 0.9077 - val_loss: 0.2892 - val_recall: 0.8956\n",
      "Epoch 120/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2187 - recall: 0.9164 - val_loss: 0.2821 - val_recall: 0.8956\n",
      "Epoch 121/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2166 - recall: 0.9150 - val_loss: 0.2744 - val_recall: 0.8991\n",
      "Epoch 122/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2181 - recall: 0.9147 - val_loss: 0.2951 - val_recall: 0.8910\n",
      "Epoch 123/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2151 - recall: 0.9169 - val_loss: 0.2670 - val_recall: 0.9044\n",
      "Epoch 124/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2079 - recall: 0.9210 - val_loss: 0.2646 - val_recall: 0.9055\n",
      "Epoch 125/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2014 - recall: 0.9202 - val_loss: 0.2436 - val_recall: 0.9125\n",
      "Epoch 126/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2066 - recall: 0.9190 - val_loss: 0.2736 - val_recall: 0.8991\n",
      "Epoch 127/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2063 - recall: 0.9193 - val_loss: 0.2603 - val_recall: 0.9061\n",
      "Epoch 128/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2079 - recall: 0.9206 - val_loss: 0.2739 - val_recall: 0.8991\n",
      "Epoch 129/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2006 - recall: 0.9203 - val_loss: 0.2686 - val_recall: 0.9038\n",
      "Epoch 130/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1973 - recall: 0.9247 - val_loss: 0.2525 - val_recall: 0.9079\n",
      "Epoch 131/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1984 - recall: 0.9262 - val_loss: 0.2547 - val_recall: 0.9073\n",
      "Epoch 132/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2029 - recall: 0.9211 - val_loss: 0.2598 - val_recall: 0.9061\n",
      "Epoch 133/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1954 - recall: 0.9268 - val_loss: 0.2543 - val_recall: 0.9067\n",
      "Epoch 134/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2003 - recall: 0.9271 - val_loss: 0.2578 - val_recall: 0.9061\n",
      "Epoch 135/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2033 - recall: 0.9212 - val_loss: 0.2584 - val_recall: 0.9055\n",
      "Epoch 136/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1955 - recall: 0.9254 - val_loss: 0.2372 - val_recall: 0.9160\n",
      "Epoch 137/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2001 - recall: 0.9240 - val_loss: 0.2564 - val_recall: 0.9073\n",
      "Epoch 138/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1970 - recall: 0.9220 - val_loss: 0.2494 - val_recall: 0.9108\n",
      "Epoch 139/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1957 - recall: 0.9255 - val_loss: 0.2697 - val_recall: 0.8985\n",
      "Epoch 140/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1900 - recall: 0.9261 - val_loss: 0.2631 - val_recall: 0.9050\n",
      "Epoch 141/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1966 - recall: 0.9234 - val_loss: 0.2661 - val_recall: 0.9044\n",
      "Epoch 142/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1948 - recall: 0.9228 - val_loss: 0.2488 - val_recall: 0.9125\n",
      "Epoch 143/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1932 - recall: 0.9265 - val_loss: 0.2659 - val_recall: 0.9026\n",
      "Epoch 144/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1919 - recall: 0.9267 - val_loss: 0.2423 - val_recall: 0.9178\n",
      "Epoch 145/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1909 - recall: 0.9287 - val_loss: 0.2310 - val_recall: 0.9195\n",
      "Epoch 146/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1856 - recall: 0.9336 - val_loss: 0.2509 - val_recall: 0.9120\n",
      "Epoch 147/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1907 - recall: 0.9259 - val_loss: 0.2573 - val_recall: 0.9073\n",
      "Epoch 148/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1883 - recall: 0.9315 - val_loss: 0.2193 - val_recall: 0.9271\n",
      "Epoch 149/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1740 - recall: 0.9368 - val_loss: 0.2201 - val_recall: 0.9265\n",
      "Epoch 150/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1711 - recall: 0.9374 - val_loss: 0.2125 - val_recall: 0.9306\n",
      "Epoch 151/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1892 - recall: 0.9313 - val_loss: 0.2298 - val_recall: 0.9230\n",
      "Epoch 152/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1767 - recall: 0.9340 - val_loss: 0.2166 - val_recall: 0.9271\n",
      "Epoch 153/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1781 - recall: 0.9346 - val_loss: 0.2406 - val_recall: 0.9160\n",
      "Epoch 154/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1759 - recall: 0.9317 - val_loss: 0.2289 - val_recall: 0.9219\n",
      "Epoch 155/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1773 - recall: 0.9337 - val_loss: 0.2488 - val_recall: 0.9120\n",
      "Epoch 156/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1841 - recall: 0.9280 - val_loss: 0.2291 - val_recall: 0.9201\n",
      "Epoch 157/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1672 - recall: 0.9368 - val_loss: 0.2175 - val_recall: 0.9271\n",
      "Epoch 158/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1769 - recall: 0.9335 - val_loss: 0.2411 - val_recall: 0.9155\n",
      "Epoch 159/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1799 - recall: 0.9305 - val_loss: 0.2444 - val_recall: 0.9131\n",
      "Epoch 160/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1714 - recall: 0.9343 - val_loss: 0.2261 - val_recall: 0.9219\n",
      "Epoch 161/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1806 - recall: 0.9350 - val_loss: 0.2381 - val_recall: 0.9172\n",
      "Epoch 162/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1803 - recall: 0.9315 - val_loss: 0.2101 - val_recall: 0.9283\n",
      "Epoch 163/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1768 - recall: 0.9333 - val_loss: 0.2277 - val_recall: 0.9207\n",
      "Epoch 164/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1651 - recall: 0.9366 - val_loss: 0.2308 - val_recall: 0.9201\n",
      "Epoch 165/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1778 - recall: 0.9296 - val_loss: 0.2032 - val_recall: 0.9324\n",
      "Epoch 166/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1690 - recall: 0.9339 - val_loss: 0.2306 - val_recall: 0.9201\n",
      "Epoch 167/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1725 - recall: 0.9336 - val_loss: 0.2406 - val_recall: 0.9143\n",
      "Epoch 168/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1662 - recall: 0.9352 - val_loss: 0.2199 - val_recall: 0.9230\n",
      "Epoch 169/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1602 - recall: 0.9383 - val_loss: 0.2001 - val_recall: 0.9359\n",
      "Epoch 170/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1766 - recall: 0.9349 - val_loss: 0.2377 - val_recall: 0.9149\n",
      "Epoch 171/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1718 - recall: 0.9338 - val_loss: 0.2072 - val_recall: 0.9300\n",
      "Epoch 172/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1645 - recall: 0.9402 - val_loss: 0.2466 - val_recall: 0.9102\n",
      "Epoch 173/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1596 - recall: 0.9392 - val_loss: 0.2153 - val_recall: 0.9265\n",
      "Epoch 174/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1629 - recall: 0.9403 - val_loss: 0.2358 - val_recall: 0.9155\n",
      "Epoch 175/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1676 - recall: 0.9415 - val_loss: 0.2287 - val_recall: 0.9201\n",
      "Epoch 176/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1659 - recall: 0.9361 - val_loss: 0.2009 - val_recall: 0.9382\n",
      "Epoch 177/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1730 - recall: 0.9361 - val_loss: 0.2106 - val_recall: 0.9324\n",
      "Epoch 178/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1577 - recall: 0.9397 - val_loss: 0.1923 - val_recall: 0.9417\n",
      "Epoch 179/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1545 - recall: 0.9439 - val_loss: 0.2179 - val_recall: 0.9294\n",
      "Epoch 180/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1590 - recall: 0.9431 - val_loss: 0.2277 - val_recall: 0.9213\n",
      "Epoch 181/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1615 - recall: 0.9391 - val_loss: 0.2196 - val_recall: 0.9254\n",
      "Epoch 182/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1606 - recall: 0.9406 - val_loss: 0.2194 - val_recall: 0.9265\n",
      "Epoch 183/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1612 - recall: 0.9436 - val_loss: 0.2047 - val_recall: 0.9359\n",
      "Epoch 184/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1544 - recall: 0.9415 - val_loss: 0.1777 - val_recall: 0.9534\n",
      "Epoch 185/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1464 - recall: 0.9463 - val_loss: 0.1903 - val_recall: 0.9464\n",
      "Epoch 186/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1581 - recall: 0.9444 - val_loss: 0.2119 - val_recall: 0.9300\n",
      "Epoch 187/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1541 - recall: 0.9429 - val_loss: 0.2114 - val_recall: 0.9324\n",
      "Epoch 188/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1536 - recall: 0.9440 - val_loss: 0.2063 - val_recall: 0.9341\n",
      "Epoch 189/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1614 - recall: 0.9405 - val_loss: 0.2237 - val_recall: 0.9248\n",
      "Epoch 190/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1520 - recall: 0.9448 - val_loss: 0.2195 - val_recall: 0.9283\n",
      "Epoch 191/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1505 - recall: 0.9456 - val_loss: 0.2016 - val_recall: 0.9376\n",
      "Epoch 192/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1450 - recall: 0.9467 - val_loss: 0.2101 - val_recall: 0.9341\n",
      "Epoch 193/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1515 - recall: 0.9443 - val_loss: 0.1920 - val_recall: 0.9434\n",
      "Epoch 194/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1510 - recall: 0.9440 - val_loss: 0.1999 - val_recall: 0.9364\n",
      "Epoch 195/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1446 - recall: 0.9518 - val_loss: 0.1904 - val_recall: 0.9440\n",
      "Epoch 196/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1436 - recall: 0.9443 - val_loss: 0.1752 - val_recall: 0.9504\n",
      "Epoch 197/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1428 - recall: 0.9507 - val_loss: 0.1908 - val_recall: 0.9452\n",
      "Epoch 198/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1505 - recall: 0.9413 - val_loss: 0.2052 - val_recall: 0.9353\n",
      "Epoch 199/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1546 - recall: 0.9472 - val_loss: 0.1984 - val_recall: 0.9394\n",
      "Epoch 200/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1446 - recall: 0.9498 - val_loss: 0.1926 - val_recall: 0.9405\n",
      "Epoch 201/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1485 - recall: 0.9453 - val_loss: 0.2093 - val_recall: 0.9347\n",
      "Epoch 202/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1474 - recall: 0.9491 - val_loss: 0.2217 - val_recall: 0.9300\n",
      "Epoch 203/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1460 - recall: 0.9453 - val_loss: 0.2043 - val_recall: 0.9341\n",
      "Epoch 204/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1384 - recall: 0.9500 - val_loss: 0.1950 - val_recall: 0.9429\n",
      "Epoch 205/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1471 - recall: 0.9470 - val_loss: 0.2098 - val_recall: 0.9364\n",
      "Epoch 206/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1472 - recall: 0.9440 - val_loss: 0.2121 - val_recall: 0.9329\n",
      "Epoch 207/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1474 - recall: 0.9467 - val_loss: 0.2068 - val_recall: 0.9341\n",
      "Epoch 208/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1386 - recall: 0.9505 - val_loss: 0.1955 - val_recall: 0.9423\n",
      "Epoch 209/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1432 - recall: 0.9512 - val_loss: 0.2019 - val_recall: 0.9388\n",
      "Epoch 210/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1391 - recall: 0.9513 - val_loss: 0.1858 - val_recall: 0.9469\n",
      "Epoch 211/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1298 - recall: 0.9559 - val_loss: 0.1915 - val_recall: 0.9446\n",
      "Epoch 212/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1328 - recall: 0.9513 - val_loss: 0.1656 - val_recall: 0.9551\n",
      "Epoch 213/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1322 - recall: 0.9562 - val_loss: 0.1861 - val_recall: 0.9458\n",
      "Epoch 214/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1368 - recall: 0.9507 - val_loss: 0.1937 - val_recall: 0.9423\n",
      "Epoch 215/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1460 - recall: 0.9476 - val_loss: 0.1840 - val_recall: 0.9458\n",
      "Epoch 216/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1361 - recall: 0.9514 - val_loss: 0.1853 - val_recall: 0.9446\n",
      "Epoch 217/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1367 - recall: 0.9517 - val_loss: 0.1664 - val_recall: 0.9563\n",
      "Epoch 218/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1321 - recall: 0.9531 - val_loss: 0.1739 - val_recall: 0.9522\n",
      "Epoch 219/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1372 - recall: 0.9523 - val_loss: 0.1698 - val_recall: 0.9545\n",
      "Epoch 220/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1392 - recall: 0.9529 - val_loss: 0.1978 - val_recall: 0.9405\n",
      "Epoch 221/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1308 - recall: 0.9547 - val_loss: 0.1677 - val_recall: 0.9551\n",
      "Epoch 222/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1317 - recall: 0.9560 - val_loss: 0.1593 - val_recall: 0.9574\n",
      "Epoch 223/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1357 - recall: 0.9506 - val_loss: 0.1813 - val_recall: 0.9481\n",
      "Epoch 224/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1335 - recall: 0.9516 - val_loss: 0.1673 - val_recall: 0.9551\n",
      "Epoch 225/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1262 - recall: 0.9568 - val_loss: 0.1640 - val_recall: 0.9586\n",
      "Epoch 226/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1277 - recall: 0.9572 - val_loss: 0.1767 - val_recall: 0.9493\n",
      "Epoch 227/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1395 - recall: 0.9536 - val_loss: 0.2003 - val_recall: 0.9388\n",
      "Epoch 228/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1375 - recall: 0.9505 - val_loss: 0.1894 - val_recall: 0.9452\n",
      "Epoch 229/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1323 - recall: 0.9525 - val_loss: 0.1690 - val_recall: 0.9557\n",
      "Epoch 230/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1355 - recall: 0.9525 - val_loss: 0.1714 - val_recall: 0.9545\n",
      "Epoch 231/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1292 - recall: 0.9551 - val_loss: 0.1884 - val_recall: 0.9452\n",
      "Epoch 232/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1311 - recall: 0.9544 - val_loss: 0.1767 - val_recall: 0.9516\n",
      "Epoch 233/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1275 - recall: 0.9528 - val_loss: 0.1645 - val_recall: 0.9580\n",
      "Epoch 234/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1352 - recall: 0.9503 - val_loss: 0.1626 - val_recall: 0.9586\n",
      "Epoch 235/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1248 - recall: 0.9560 - val_loss: 0.1646 - val_recall: 0.9574\n",
      "Epoch 236/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1284 - recall: 0.9587 - val_loss: 0.1838 - val_recall: 0.9504\n",
      "Epoch 237/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1344 - recall: 0.9536 - val_loss: 0.1904 - val_recall: 0.9452\n",
      "Epoch 238/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1322 - recall: 0.9536 - val_loss: 0.1800 - val_recall: 0.9475\n",
      "Epoch 239/600\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.1232 - recall: 0.9569 - val_loss: 0.1690 - val_recall: 0.9534\n",
      "Epoch 240/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1276 - recall: 0.9547 - val_loss: 0.1677 - val_recall: 0.9563\n",
      "Epoch 241/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1359 - recall: 0.9527 - val_loss: 0.1772 - val_recall: 0.9522\n",
      "Epoch 242/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1276 - recall: 0.9564 - val_loss: 0.1667 - val_recall: 0.9551\n",
      "Epoch 243/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1305 - recall: 0.9577 - val_loss: 0.1913 - val_recall: 0.9434\n",
      "Epoch 244/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1289 - recall: 0.9564 - val_loss: 0.1875 - val_recall: 0.9446\n",
      "Epoch 245/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1246 - recall: 0.9578 - val_loss: 0.1531 - val_recall: 0.9592\n",
      "Epoch 246/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1171 - recall: 0.9606 - val_loss: 0.1665 - val_recall: 0.9574\n",
      "Epoch 247/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1251 - recall: 0.9584 - val_loss: 0.1597 - val_recall: 0.9574\n",
      "Epoch 248/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1162 - recall: 0.9612 - val_loss: 0.1673 - val_recall: 0.9569\n",
      "Epoch 249/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1209 - recall: 0.9608 - val_loss: 0.1630 - val_recall: 0.9592\n",
      "Epoch 250/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1197 - recall: 0.9599 - val_loss: 0.1615 - val_recall: 0.9574\n",
      "Epoch 251/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1267 - recall: 0.9593 - val_loss: 0.1720 - val_recall: 0.9528\n",
      "Epoch 252/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1143 - recall: 0.9621 - val_loss: 0.1524 - val_recall: 0.9615\n",
      "Epoch 253/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1209 - recall: 0.9599 - val_loss: 0.1641 - val_recall: 0.9569\n",
      "Epoch 254/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1159 - recall: 0.9601 - val_loss: 0.1855 - val_recall: 0.9464\n",
      "Epoch 255/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1271 - recall: 0.9572 - val_loss: 0.1835 - val_recall: 0.9493\n",
      "Epoch 256/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1181 - recall: 0.9589 - val_loss: 0.1579 - val_recall: 0.9580\n",
      "Epoch 257/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1156 - recall: 0.9596 - val_loss: 0.1573 - val_recall: 0.9569\n",
      "Epoch 258/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1203 - recall: 0.9600 - val_loss: 0.1455 - val_recall: 0.9633\n",
      "Epoch 259/600\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.1173 - recall: 0.9626 - val_loss: 0.1421 - val_recall: 0.9638\n",
      "Epoch 260/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1193 - recall: 0.9615 - val_loss: 0.1764 - val_recall: 0.9510\n",
      "Epoch 261/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1195 - recall: 0.9593 - val_loss: 0.1491 - val_recall: 0.9615\n",
      "Epoch 262/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1171 - recall: 0.9619 - val_loss: 0.1688 - val_recall: 0.9545\n",
      "Epoch 263/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1188 - recall: 0.9602 - val_loss: 0.1501 - val_recall: 0.9621\n",
      "Epoch 264/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1172 - recall: 0.9603 - val_loss: 0.1633 - val_recall: 0.9557\n",
      "Epoch 265/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1149 - recall: 0.9625 - val_loss: 0.1876 - val_recall: 0.9481\n",
      "Epoch 266/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1196 - recall: 0.9583 - val_loss: 0.1678 - val_recall: 0.9539\n",
      "Epoch 267/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1092 - recall: 0.9647 - val_loss: 0.1642 - val_recall: 0.9557\n",
      "Epoch 268/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1236 - recall: 0.9581 - val_loss: 0.1782 - val_recall: 0.9510\n",
      "Epoch 269/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1205 - recall: 0.9603 - val_loss: 0.1588 - val_recall: 0.9603\n",
      "Epoch 270/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1157 - recall: 0.9610 - val_loss: 0.1867 - val_recall: 0.9469\n",
      "Epoch 271/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1098 - recall: 0.9647 - val_loss: 0.1758 - val_recall: 0.9499\n",
      "Epoch 272/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1121 - recall: 0.9615 - val_loss: 0.1645 - val_recall: 0.9539\n",
      "Epoch 273/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1106 - recall: 0.9633 - val_loss: 0.1477 - val_recall: 0.9638\n",
      "Epoch 274/600\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.1088 - recall: 0.9645 - val_loss: 0.1416 - val_recall: 0.9703\n",
      "Epoch 275/600\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.1324 - recall: 0.9533 - val_loss: 0.1777 - val_recall: 0.9504\n",
      "Epoch 276/600\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.1078 - recall: 0.9653 - val_loss: 0.1631 - val_recall: 0.9551\n",
      "Epoch 277/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1045 - recall: 0.9656 - val_loss: 0.1434 - val_recall: 0.9679\n",
      "Epoch 278/600\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.1148 - recall: 0.9612 - val_loss: 0.1603 - val_recall: 0.9586\n",
      "Epoch 279/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1108 - recall: 0.9629 - val_loss: 0.1521 - val_recall: 0.9627\n",
      "Epoch 280/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1121 - recall: 0.9618 - val_loss: 0.1682 - val_recall: 0.9557\n",
      "Epoch 281/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1126 - recall: 0.9620 - val_loss: 0.1626 - val_recall: 0.9603\n",
      "Epoch 282/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1113 - recall: 0.9639 - val_loss: 0.1842 - val_recall: 0.9504\n",
      "Epoch 283/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1165 - recall: 0.9653 - val_loss: 0.1957 - val_recall: 0.9440\n",
      "Epoch 284/600\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.1120 - recall: 0.9609 - val_loss: 0.1745 - val_recall: 0.9545\n",
      "Epoch 285/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1185 - recall: 0.9639 - val_loss: 0.1662 - val_recall: 0.9592\n",
      "Epoch 286/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1147 - recall: 0.9619 - val_loss: 0.1482 - val_recall: 0.9633\n",
      "Epoch 287/600\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.1259 - recall: 0.9606 - val_loss: 0.1736 - val_recall: 0.9516\n",
      "Epoch 288/600\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.1183 - recall: 0.9621 - val_loss: 0.2033 - val_recall: 0.9446\n",
      "Epoch 289/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1060 - recall: 0.9643 - val_loss: 0.1339 - val_recall: 0.9703\n",
      "Epoch 290/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1161 - recall: 0.9619 - val_loss: 0.1701 - val_recall: 0.9545\n",
      "Epoch 291/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1203 - recall: 0.9622 - val_loss: 0.1565 - val_recall: 0.9598\n",
      "Epoch 292/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1110 - recall: 0.9638 - val_loss: 0.1641 - val_recall: 0.9569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 293/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1090 - recall: 0.9649 - val_loss: 0.1643 - val_recall: 0.9551\n",
      "Epoch 294/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1103 - recall: 0.9640 - val_loss: 0.1554 - val_recall: 0.9609\n",
      "Epoch 295/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1101 - recall: 0.9659 - val_loss: 0.1733 - val_recall: 0.9534\n",
      "Epoch 296/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1124 - recall: 0.9626 - val_loss: 0.1445 - val_recall: 0.9638\n",
      "Epoch 297/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0997 - recall: 0.9675 - val_loss: 0.1251 - val_recall: 0.9732\n",
      "Epoch 298/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1065 - recall: 0.9678 - val_loss: 0.1480 - val_recall: 0.9633\n",
      "Epoch 299/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1033 - recall: 0.9667 - val_loss: 0.1469 - val_recall: 0.9609\n",
      "Epoch 300/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1042 - recall: 0.9668 - val_loss: 0.1593 - val_recall: 0.9569\n",
      "Epoch 301/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1017 - recall: 0.9671 - val_loss: 0.1432 - val_recall: 0.9638\n",
      "Epoch 302/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1081 - recall: 0.9652 - val_loss: 0.1414 - val_recall: 0.9662\n",
      "Epoch 303/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1039 - recall: 0.9675 - val_loss: 0.1516 - val_recall: 0.9592\n",
      "Epoch 304/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1031 - recall: 0.9658 - val_loss: 0.1431 - val_recall: 0.9656\n",
      "Epoch 305/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1046 - recall: 0.9682 - val_loss: 0.1388 - val_recall: 0.9668\n",
      "Epoch 306/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1083 - recall: 0.9646 - val_loss: 0.1665 - val_recall: 0.9563\n",
      "Epoch 307/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1133 - recall: 0.9638 - val_loss: 0.1398 - val_recall: 0.9668\n",
      "Epoch 308/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1046 - recall: 0.9659 - val_loss: 0.1410 - val_recall: 0.9656\n",
      "Epoch 309/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1013 - recall: 0.9684 - val_loss: 0.1542 - val_recall: 0.9621\n",
      "Epoch 310/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1026 - recall: 0.9675 - val_loss: 0.1450 - val_recall: 0.9668\n",
      "Epoch 311/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1046 - recall: 0.9673 - val_loss: 0.1338 - val_recall: 0.9703\n",
      "Epoch 312/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0952 - recall: 0.9712 - val_loss: 0.1257 - val_recall: 0.9755\n",
      "Epoch 313/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1045 - recall: 0.9682 - val_loss: 0.1517 - val_recall: 0.9638\n",
      "Epoch 314/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1067 - recall: 0.9661 - val_loss: 0.1423 - val_recall: 0.9650\n",
      "Epoch 315/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1084 - recall: 0.9647 - val_loss: 0.1299 - val_recall: 0.9726\n",
      "Epoch 316/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1034 - recall: 0.9694 - val_loss: 0.1406 - val_recall: 0.9708\n",
      "Epoch 317/600\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.1169 - recall: 0.9593 - val_loss: 0.1250 - val_recall: 0.9738\n",
      "Epoch 318/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0955 - recall: 0.9703 - val_loss: 0.1259 - val_recall: 0.9714\n",
      "Epoch 319/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0987 - recall: 0.9692 - val_loss: 0.1332 - val_recall: 0.9703\n",
      "Epoch 320/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1068 - recall: 0.9665 - val_loss: 0.1539 - val_recall: 0.9638\n",
      "Epoch 321/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1042 - recall: 0.9671 - val_loss: 0.1345 - val_recall: 0.9714\n",
      "Epoch 322/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1052 - recall: 0.9672 - val_loss: 0.1614 - val_recall: 0.9609\n",
      "Epoch 323/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.0999 - recall: 0.9691 - val_loss: 0.1339 - val_recall: 0.9714\n",
      "Epoch 324/600\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.1078 - recall: 0.9680 - val_loss: 0.1444 - val_recall: 0.9685\n",
      "Epoch 325/600\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 0.1010 - recall: 0.9662 - val_loss: 0.1506 - val_recall: 0.9644\n",
      "Epoch 326/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1037 - recall: 0.9675 - val_loss: 0.1585 - val_recall: 0.9574\n",
      "Epoch 327/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1053 - recall: 0.9662 - val_loss: 0.1525 - val_recall: 0.9603\n",
      "Epoch 328/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0996 - recall: 0.9691 - val_loss: 0.1418 - val_recall: 0.9644\n",
      "Epoch 329/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1024 - recall: 0.9691 - val_loss: 0.1612 - val_recall: 0.9580\n",
      "Epoch 330/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1002 - recall: 0.9677 - val_loss: 0.1335 - val_recall: 0.9697\n",
      "Epoch 331/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1120 - recall: 0.9652 - val_loss: 0.1799 - val_recall: 0.9516\n",
      "Epoch 332/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1024 - recall: 0.9713 - val_loss: 0.1514 - val_recall: 0.9656\n",
      "Epoch 333/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0965 - recall: 0.9706 - val_loss: 0.1413 - val_recall: 0.9679\n",
      "Epoch 334/600\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 0.0963 - recall: 0.9700 - val_loss: 0.1275 - val_recall: 0.9749\n",
      "Epoch 335/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0948 - recall: 0.9709 - val_loss: 0.1378 - val_recall: 0.9691\n",
      "Epoch 336/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0959 - recall: 0.9692 - val_loss: 0.1285 - val_recall: 0.9732\n",
      "Epoch 337/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0953 - recall: 0.9704 - val_loss: 0.1313 - val_recall: 0.9703\n",
      "Epoch 338/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1035 - recall: 0.9681 - val_loss: 0.1461 - val_recall: 0.9668\n",
      "Epoch 339/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0989 - recall: 0.9684 - val_loss: 0.1341 - val_recall: 0.9708\n",
      "Epoch 340/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0898 - recall: 0.9740 - val_loss: 0.1273 - val_recall: 0.9732\n",
      "Epoch 341/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1002 - recall: 0.9683 - val_loss: 0.1271 - val_recall: 0.9738\n",
      "Epoch 342/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1012 - recall: 0.9720 - val_loss: 0.1665 - val_recall: 0.9569\n",
      "Epoch 343/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0935 - recall: 0.9701 - val_loss: 0.1124 - val_recall: 0.9767\n",
      "Epoch 344/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1004 - recall: 0.9704 - val_loss: 0.1429 - val_recall: 0.9685\n",
      "Epoch 345/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0961 - recall: 0.9694 - val_loss: 0.1334 - val_recall: 0.9720\n",
      "Epoch 346/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0959 - recall: 0.9721 - val_loss: 0.1500 - val_recall: 0.9679\n",
      "Epoch 347/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0994 - recall: 0.9702 - val_loss: 0.1583 - val_recall: 0.9615\n",
      "Epoch 348/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1020 - recall: 0.9682 - val_loss: 0.1605 - val_recall: 0.9603\n",
      "Epoch 349/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0887 - recall: 0.9732 - val_loss: 0.1320 - val_recall: 0.9720\n",
      "Epoch 350/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0987 - recall: 0.9687 - val_loss: 0.1434 - val_recall: 0.9656\n",
      "Epoch 351/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0974 - recall: 0.9708 - val_loss: 0.1402 - val_recall: 0.9662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0923 - recall: 0.9720 - val_loss: 0.1180 - val_recall: 0.9773\n",
      "Epoch 353/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1020 - recall: 0.9736 - val_loss: 0.1359 - val_recall: 0.9685\n",
      "Epoch 354/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0929 - recall: 0.9691 - val_loss: 0.1158 - val_recall: 0.9761\n",
      "Epoch 355/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0976 - recall: 0.9698 - val_loss: 0.1193 - val_recall: 0.9749\n",
      "Epoch 356/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0964 - recall: 0.9696 - val_loss: 0.1136 - val_recall: 0.9773\n",
      "Epoch 357/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.1037 - recall: 0.9661 - val_loss: 0.1454 - val_recall: 0.9656\n",
      "Epoch 358/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0969 - recall: 0.9705 - val_loss: 0.1390 - val_recall: 0.9697\n",
      "Epoch 359/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0989 - recall: 0.9687 - val_loss: 0.1369 - val_recall: 0.9679\n",
      "Epoch 360/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0922 - recall: 0.9721 - val_loss: 0.1214 - val_recall: 0.9749\n",
      "Epoch 361/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0876 - recall: 0.9738 - val_loss: 0.1212 - val_recall: 0.9732\n",
      "Epoch 362/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0969 - recall: 0.9721 - val_loss: 0.1520 - val_recall: 0.9633\n",
      "Epoch 363/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0918 - recall: 0.9720 - val_loss: 0.1301 - val_recall: 0.9732\n",
      "Epoch 364/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0931 - recall: 0.9722 - val_loss: 0.1309 - val_recall: 0.9714\n",
      "Epoch 365/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0908 - recall: 0.9734 - val_loss: 0.1181 - val_recall: 0.9761\n",
      "Epoch 366/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0882 - recall: 0.9737 - val_loss: 0.1242 - val_recall: 0.9738\n",
      "Epoch 367/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0904 - recall: 0.9748 - val_loss: 0.1292 - val_recall: 0.9703\n",
      "Epoch 368/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0894 - recall: 0.9754 - val_loss: 0.1288 - val_recall: 0.9726\n",
      "Epoch 369/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0951 - recall: 0.9734 - val_loss: 0.1464 - val_recall: 0.9644\n",
      "Epoch 370/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.1000 - recall: 0.9692 - val_loss: 0.1507 - val_recall: 0.9621\n",
      "Epoch 371/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0925 - recall: 0.9731 - val_loss: 0.1349 - val_recall: 0.9697\n",
      "Epoch 372/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0978 - recall: 0.9681 - val_loss: 0.1654 - val_recall: 0.9598\n",
      "Epoch 373/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0933 - recall: 0.9714 - val_loss: 0.1324 - val_recall: 0.9714\n",
      "Epoch 374/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0851 - recall: 0.9757 - val_loss: 0.1318 - val_recall: 0.9703\n",
      "Epoch 375/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0946 - recall: 0.9711 - val_loss: 0.1344 - val_recall: 0.9691\n",
      "Epoch 376/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0908 - recall: 0.9723 - val_loss: 0.1310 - val_recall: 0.9726\n",
      "Epoch 377/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0915 - recall: 0.9743 - val_loss: 0.1361 - val_recall: 0.9685\n",
      "Epoch 378/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0979 - recall: 0.9702 - val_loss: 0.1313 - val_recall: 0.9726\n",
      "Epoch 379/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0961 - recall: 0.9731 - val_loss: 0.1478 - val_recall: 0.9627\n",
      "Epoch 380/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0938 - recall: 0.9719 - val_loss: 0.1421 - val_recall: 0.9685\n",
      "Epoch 381/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0883 - recall: 0.9745 - val_loss: 0.1385 - val_recall: 0.9691\n",
      "Epoch 382/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0871 - recall: 0.9748 - val_loss: 0.1451 - val_recall: 0.9662\n",
      "Epoch 383/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0910 - recall: 0.9732 - val_loss: 0.1529 - val_recall: 0.9650\n",
      "Epoch 384/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0886 - recall: 0.9757 - val_loss: 0.1241 - val_recall: 0.9749\n",
      "Epoch 385/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0960 - recall: 0.9722 - val_loss: 0.1392 - val_recall: 0.9703\n",
      "Epoch 386/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0886 - recall: 0.9760 - val_loss: 0.1239 - val_recall: 0.9749\n",
      "Epoch 387/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0893 - recall: 0.9730 - val_loss: 0.1293 - val_recall: 0.9732\n",
      "Epoch 388/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0922 - recall: 0.9694 - val_loss: 0.1298 - val_recall: 0.9732\n",
      "Epoch 389/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0880 - recall: 0.9734 - val_loss: 0.1291 - val_recall: 0.9743\n",
      "Epoch 390/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0927 - recall: 0.9741 - val_loss: 0.1479 - val_recall: 0.9662\n",
      "Epoch 391/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0882 - recall: 0.9732 - val_loss: 0.1273 - val_recall: 0.9714\n",
      "Epoch 392/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0834 - recall: 0.9762 - val_loss: 0.1182 - val_recall: 0.9755\n",
      "Epoch 393/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0967 - recall: 0.9738 - val_loss: 0.1559 - val_recall: 0.9627\n",
      "Epoch 394/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0995 - recall: 0.9731 - val_loss: 0.2095 - val_recall: 0.9394\n",
      "Epoch 395/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0881 - recall: 0.9721 - val_loss: 0.1104 - val_recall: 0.9773\n",
      "Epoch 396/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0900 - recall: 0.9749 - val_loss: 0.1333 - val_recall: 0.9714\n",
      "Epoch 397/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0870 - recall: 0.9745 - val_loss: 0.1253 - val_recall: 0.9732\n",
      "Epoch 398/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0941 - recall: 0.9739 - val_loss: 0.1356 - val_recall: 0.9714\n",
      "Epoch 399/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0875 - recall: 0.9730 - val_loss: 0.1241 - val_recall: 0.9732\n",
      "Epoch 400/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0881 - recall: 0.9748 - val_loss: 0.1230 - val_recall: 0.9738\n",
      "Epoch 401/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0845 - recall: 0.9762 - val_loss: 0.1157 - val_recall: 0.9749\n",
      "Epoch 402/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0898 - recall: 0.9747 - val_loss: 0.1178 - val_recall: 0.9761\n",
      "Epoch 403/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0831 - recall: 0.9761 - val_loss: 0.1176 - val_recall: 0.9749\n",
      "Epoch 404/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0867 - recall: 0.9749 - val_loss: 0.1219 - val_recall: 0.9743\n",
      "Epoch 405/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0801 - recall: 0.9788 - val_loss: 0.1348 - val_recall: 0.9703\n",
      "Epoch 406/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0758 - recall: 0.9801 - val_loss: 0.1194 - val_recall: 0.9749\n",
      "Epoch 407/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0825 - recall: 0.9755 - val_loss: 0.1168 - val_recall: 0.9761\n",
      "Epoch 408/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0870 - recall: 0.9749 - val_loss: 0.1111 - val_recall: 0.9778\n",
      "Epoch 409/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0863 - recall: 0.9765 - val_loss: 0.1247 - val_recall: 0.9726\n",
      "Epoch 410/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0870 - recall: 0.9760 - val_loss: 0.1128 - val_recall: 0.9773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 411/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0849 - recall: 0.9760 - val_loss: 0.1148 - val_recall: 0.9755\n",
      "Epoch 412/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0787 - recall: 0.9789 - val_loss: 0.1135 - val_recall: 0.9749\n",
      "Epoch 413/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0834 - recall: 0.9771 - val_loss: 0.1458 - val_recall: 0.9638\n",
      "Epoch 414/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0846 - recall: 0.9757 - val_loss: 0.1096 - val_recall: 0.9778\n",
      "Epoch 415/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0829 - recall: 0.9784 - val_loss: 0.1208 - val_recall: 0.9743\n",
      "Epoch 416/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0865 - recall: 0.9754 - val_loss: 0.1149 - val_recall: 0.9761\n",
      "Epoch 417/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0808 - recall: 0.9771 - val_loss: 0.1076 - val_recall: 0.9767\n",
      "Epoch 418/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0878 - recall: 0.9759 - val_loss: 0.1147 - val_recall: 0.9755\n",
      "Epoch 419/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0843 - recall: 0.9759 - val_loss: 0.1280 - val_recall: 0.9743\n",
      "Epoch 420/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0827 - recall: 0.9766 - val_loss: 0.1150 - val_recall: 0.9773\n",
      "Epoch 421/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0816 - recall: 0.9774 - val_loss: 0.1104 - val_recall: 0.9773\n",
      "Epoch 422/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0807 - recall: 0.9758 - val_loss: 0.1154 - val_recall: 0.9767\n",
      "Epoch 423/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0902 - recall: 0.9746 - val_loss: 0.1217 - val_recall: 0.9761\n",
      "Epoch 424/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0801 - recall: 0.9788 - val_loss: 0.1360 - val_recall: 0.9708\n",
      "Epoch 425/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0777 - recall: 0.9779 - val_loss: 0.1027 - val_recall: 0.9778\n",
      "Epoch 426/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0835 - recall: 0.9774 - val_loss: 0.1455 - val_recall: 0.9691\n",
      "Epoch 427/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0765 - recall: 0.9793 - val_loss: 0.1202 - val_recall: 0.9761\n",
      "Epoch 428/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0827 - recall: 0.9774 - val_loss: 0.1051 - val_recall: 0.9790\n",
      "Epoch 429/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0844 - recall: 0.9766 - val_loss: 0.1420 - val_recall: 0.9650\n",
      "Epoch 430/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0960 - recall: 0.9711 - val_loss: 0.1060 - val_recall: 0.9796\n",
      "Epoch 431/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0802 - recall: 0.9796 - val_loss: 0.1126 - val_recall: 0.9773\n",
      "Epoch 432/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0829 - recall: 0.9773 - val_loss: 0.1157 - val_recall: 0.9738\n",
      "Epoch 433/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0896 - recall: 0.9729 - val_loss: 0.1239 - val_recall: 0.9743\n",
      "Epoch 434/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0773 - recall: 0.9796 - val_loss: 0.1103 - val_recall: 0.9778\n",
      "Epoch 435/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0860 - recall: 0.9762 - val_loss: 0.1573 - val_recall: 0.9586\n",
      "Epoch 436/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0841 - recall: 0.9740 - val_loss: 0.1220 - val_recall: 0.9749\n",
      "Epoch 437/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0879 - recall: 0.9773 - val_loss: 0.1176 - val_recall: 0.9761\n",
      "Epoch 438/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0808 - recall: 0.9766 - val_loss: 0.1090 - val_recall: 0.9784\n",
      "Epoch 439/600\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 0.0854 - recall: 0.9771 - val_loss: 0.1250 - val_recall: 0.9743\n",
      "Epoch 440/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0745 - recall: 0.9802 - val_loss: 0.1071 - val_recall: 0.9778\n",
      "Epoch 441/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0825 - recall: 0.9783 - val_loss: 0.1242 - val_recall: 0.9749\n",
      "Epoch 442/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0833 - recall: 0.9788 - val_loss: 0.1263 - val_recall: 0.9743\n",
      "Epoch 443/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0829 - recall: 0.9768 - val_loss: 0.1149 - val_recall: 0.9773\n",
      "Epoch 444/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0778 - recall: 0.9784 - val_loss: 0.1229 - val_recall: 0.9761\n",
      "Epoch 445/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0791 - recall: 0.9785 - val_loss: 0.1169 - val_recall: 0.9738\n",
      "Epoch 446/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0836 - recall: 0.9762 - val_loss: 0.1117 - val_recall: 0.9773\n",
      "Epoch 447/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0800 - recall: 0.9787 - val_loss: 0.1261 - val_recall: 0.9743\n",
      "Epoch 448/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0856 - recall: 0.9749 - val_loss: 0.1338 - val_recall: 0.9726\n",
      "Epoch 449/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0841 - recall: 0.9760 - val_loss: 0.1130 - val_recall: 0.9755\n",
      "Epoch 450/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0786 - recall: 0.9792 - val_loss: 0.1396 - val_recall: 0.9668\n",
      "Epoch 451/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0826 - recall: 0.9758 - val_loss: 0.1468 - val_recall: 0.9662\n",
      "Epoch 452/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0804 - recall: 0.9789 - val_loss: 0.1492 - val_recall: 0.9644\n",
      "Epoch 453/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0849 - recall: 0.9776 - val_loss: 0.1516 - val_recall: 0.9644\n",
      "Epoch 454/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0802 - recall: 0.9769 - val_loss: 0.1217 - val_recall: 0.9749\n",
      "Epoch 455/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0792 - recall: 0.9794 - val_loss: 0.1160 - val_recall: 0.9761\n",
      "Epoch 456/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0758 - recall: 0.9793 - val_loss: 0.1108 - val_recall: 0.9773\n",
      "Epoch 457/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0830 - recall: 0.9793 - val_loss: 0.1598 - val_recall: 0.9598\n",
      "Epoch 458/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0787 - recall: 0.9778 - val_loss: 0.1258 - val_recall: 0.9732\n",
      "Epoch 459/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0777 - recall: 0.9795 - val_loss: 0.1150 - val_recall: 0.9755\n",
      "Epoch 460/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0884 - recall: 0.9767 - val_loss: 0.1349 - val_recall: 0.9726\n",
      "Epoch 461/600\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 0.0780 - recall: 0.9785 - val_loss: 0.1168 - val_recall: 0.9761\n",
      "Epoch 462/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0760 - recall: 0.9816 - val_loss: 0.1292 - val_recall: 0.9743\n",
      "Epoch 463/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0802 - recall: 0.9792 - val_loss: 0.1145 - val_recall: 0.9767\n",
      "Epoch 464/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0732 - recall: 0.9805 - val_loss: 0.1091 - val_recall: 0.9773\n",
      "Epoch 465/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0889 - recall: 0.9750 - val_loss: 0.1122 - val_recall: 0.9767\n",
      "Epoch 466/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0759 - recall: 0.9812 - val_loss: 0.1168 - val_recall: 0.9767\n",
      "Epoch 467/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0788 - recall: 0.9785 - val_loss: 0.1242 - val_recall: 0.9761\n",
      "Epoch 468/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0754 - recall: 0.9801 - val_loss: 0.1089 - val_recall: 0.9796\n",
      "Epoch 469/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0746 - recall: 0.9814 - val_loss: 0.1224 - val_recall: 0.9761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0832 - recall: 0.9749 - val_loss: 0.1189 - val_recall: 0.9749\n",
      "Epoch 471/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0809 - recall: 0.9804 - val_loss: 0.1219 - val_recall: 0.9749\n",
      "Epoch 472/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0726 - recall: 0.9814 - val_loss: 0.1128 - val_recall: 0.9767\n",
      "Epoch 473/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0780 - recall: 0.9805 - val_loss: 0.1194 - val_recall: 0.9755\n",
      "Epoch 474/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0771 - recall: 0.9792 - val_loss: 0.1129 - val_recall: 0.9767\n",
      "Epoch 475/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0738 - recall: 0.9807 - val_loss: 0.1242 - val_recall: 0.9749\n",
      "Epoch 476/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0786 - recall: 0.9807 - val_loss: 0.1627 - val_recall: 0.9615\n",
      "Epoch 477/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0746 - recall: 0.9817 - val_loss: 0.1254 - val_recall: 0.9732\n",
      "Epoch 478/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0817 - recall: 0.9769 - val_loss: 0.1255 - val_recall: 0.9720\n",
      "Epoch 479/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0729 - recall: 0.9798 - val_loss: 0.1139 - val_recall: 0.9784\n",
      "Epoch 480/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0819 - recall: 0.9784 - val_loss: 0.1288 - val_recall: 0.9726\n",
      "Epoch 481/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0825 - recall: 0.9797 - val_loss: 0.1369 - val_recall: 0.9703\n",
      "Epoch 482/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0762 - recall: 0.9805 - val_loss: 0.1294 - val_recall: 0.9726\n",
      "Epoch 483/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0719 - recall: 0.9825 - val_loss: 0.1209 - val_recall: 0.9761\n",
      "Epoch 484/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0835 - recall: 0.9783 - val_loss: 0.1607 - val_recall: 0.9592\n",
      "Epoch 485/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0845 - recall: 0.9755 - val_loss: 0.1148 - val_recall: 0.9778\n",
      "Epoch 486/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0781 - recall: 0.9799 - val_loss: 0.1210 - val_recall: 0.9761\n",
      "Epoch 487/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0705 - recall: 0.9820 - val_loss: 0.1209 - val_recall: 0.9743\n",
      "Epoch 488/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0787 - recall: 0.9793 - val_loss: 0.1263 - val_recall: 0.9749\n",
      "Epoch 489/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0775 - recall: 0.9789 - val_loss: 0.1105 - val_recall: 0.9778\n",
      "Epoch 490/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0798 - recall: 0.9786 - val_loss: 0.1365 - val_recall: 0.9708\n",
      "Epoch 491/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0788 - recall: 0.9793 - val_loss: 0.1296 - val_recall: 0.9714\n",
      "Epoch 492/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0753 - recall: 0.9808 - val_loss: 0.1321 - val_recall: 0.9738\n",
      "Epoch 493/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0762 - recall: 0.9794 - val_loss: 0.1223 - val_recall: 0.9767\n",
      "Epoch 494/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0824 - recall: 0.9785 - val_loss: 0.1221 - val_recall: 0.9720\n",
      "Epoch 495/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0746 - recall: 0.9808 - val_loss: 0.1095 - val_recall: 0.9761\n",
      "Epoch 496/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0680 - recall: 0.9821 - val_loss: 0.1063 - val_recall: 0.9773\n",
      "Epoch 497/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0758 - recall: 0.9801 - val_loss: 0.1174 - val_recall: 0.9749\n",
      "Epoch 498/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0868 - recall: 0.9765 - val_loss: 0.1187 - val_recall: 0.9761\n",
      "Epoch 499/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0823 - recall: 0.9764 - val_loss: 0.1231 - val_recall: 0.9738\n",
      "Epoch 500/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0784 - recall: 0.9798 - val_loss: 0.1312 - val_recall: 0.9743\n",
      "Epoch 501/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0779 - recall: 0.9801 - val_loss: 0.1203 - val_recall: 0.9761\n",
      "Epoch 502/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0750 - recall: 0.9811 - val_loss: 0.1291 - val_recall: 0.9720\n",
      "Epoch 503/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0689 - recall: 0.9834 - val_loss: 0.1245 - val_recall: 0.9761\n",
      "Epoch 504/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0760 - recall: 0.9805 - val_loss: 0.1190 - val_recall: 0.9755\n",
      "Epoch 505/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0707 - recall: 0.9831 - val_loss: 0.1267 - val_recall: 0.9738\n",
      "Epoch 506/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0797 - recall: 0.9779 - val_loss: 0.1145 - val_recall: 0.9767\n",
      "Epoch 507/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0708 - recall: 0.9851 - val_loss: 0.1160 - val_recall: 0.9767\n",
      "Epoch 508/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0731 - recall: 0.9813 - val_loss: 0.1207 - val_recall: 0.9749\n",
      "Epoch 509/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0753 - recall: 0.9805 - val_loss: 0.1183 - val_recall: 0.9755\n",
      "Epoch 510/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0823 - recall: 0.9795 - val_loss: 0.1117 - val_recall: 0.9761\n",
      "Epoch 511/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0752 - recall: 0.9794 - val_loss: 0.1252 - val_recall: 0.9732\n",
      "Epoch 512/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0751 - recall: 0.9803 - val_loss: 0.1215 - val_recall: 0.9761\n",
      "Epoch 513/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0717 - recall: 0.9826 - val_loss: 0.1199 - val_recall: 0.9767\n",
      "Epoch 514/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0753 - recall: 0.9810 - val_loss: 0.1244 - val_recall: 0.9755\n",
      "Epoch 515/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0719 - recall: 0.9838 - val_loss: 0.1193 - val_recall: 0.9755\n",
      "Epoch 516/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0768 - recall: 0.9799 - val_loss: 0.1332 - val_recall: 0.9703\n",
      "Epoch 517/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0783 - recall: 0.9783 - val_loss: 0.1131 - val_recall: 0.9761\n",
      "Epoch 518/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0748 - recall: 0.9817 - val_loss: 0.1222 - val_recall: 0.9749\n",
      "Epoch 519/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0726 - recall: 0.9796 - val_loss: 0.1120 - val_recall: 0.9790\n",
      "Epoch 520/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0721 - recall: 0.9795 - val_loss: 0.1103 - val_recall: 0.9778\n",
      "Epoch 521/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0727 - recall: 0.9810 - val_loss: 0.1119 - val_recall: 0.9784\n",
      "Epoch 522/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0736 - recall: 0.9824 - val_loss: 0.1221 - val_recall: 0.9749\n",
      "Epoch 523/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0781 - recall: 0.9802 - val_loss: 0.1208 - val_recall: 0.9767\n",
      "Epoch 524/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0703 - recall: 0.9825 - val_loss: 0.1250 - val_recall: 0.9755\n",
      "Epoch 525/600\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 0.0705 - recall: 0.9821 - val_loss: 0.1086 - val_recall: 0.9778\n",
      "Epoch 526/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0684 - recall: 0.9842 - val_loss: 0.1202 - val_recall: 0.9767\n",
      "Epoch 527/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0689 - recall: 0.9841 - val_loss: 0.1187 - val_recall: 0.9761\n",
      "Epoch 528/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0660 - recall: 0.9853 - val_loss: 0.1108 - val_recall: 0.9790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 529/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0766 - recall: 0.9802 - val_loss: 0.1228 - val_recall: 0.9784\n",
      "Epoch 530/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0736 - recall: 0.9824 - val_loss: 0.1272 - val_recall: 0.9755\n",
      "Epoch 531/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0697 - recall: 0.9834 - val_loss: 0.1207 - val_recall: 0.9761\n",
      "Epoch 532/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0734 - recall: 0.9817 - val_loss: 0.1101 - val_recall: 0.9778\n",
      "Epoch 533/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0659 - recall: 0.9844 - val_loss: 0.1104 - val_recall: 0.9790\n",
      "Epoch 534/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0693 - recall: 0.9836 - val_loss: 0.1116 - val_recall: 0.9784\n",
      "Epoch 535/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0752 - recall: 0.9815 - val_loss: 0.1279 - val_recall: 0.9755\n",
      "Epoch 536/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0704 - recall: 0.9824 - val_loss: 0.1231 - val_recall: 0.9767\n",
      "Epoch 537/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0691 - recall: 0.9827 - val_loss: 0.1134 - val_recall: 0.9778\n",
      "Epoch 538/600\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 0.0902 - recall: 0.9786 - val_loss: 0.1383 - val_recall: 0.9703\n",
      "Epoch 539/600\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 0.0683 - recall: 0.9838 - val_loss: 0.1195 - val_recall: 0.9749\n",
      "Epoch 540/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0709 - recall: 0.9832 - val_loss: 0.1247 - val_recall: 0.9749\n",
      "Epoch 541/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0705 - recall: 0.9811 - val_loss: 0.1147 - val_recall: 0.9767\n",
      "Epoch 542/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0717 - recall: 0.9820 - val_loss: 0.1213 - val_recall: 0.9761\n",
      "Epoch 543/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0757 - recall: 0.9795 - val_loss: 0.1259 - val_recall: 0.9743\n",
      "Epoch 544/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0726 - recall: 0.9804 - val_loss: 0.1060 - val_recall: 0.9802\n",
      "Epoch 545/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0735 - recall: 0.9816 - val_loss: 0.1140 - val_recall: 0.9778\n",
      "Epoch 546/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0722 - recall: 0.9798 - val_loss: 0.1021 - val_recall: 0.9808\n",
      "Epoch 547/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0677 - recall: 0.9832 - val_loss: 0.1116 - val_recall: 0.9796\n",
      "Epoch 548/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0702 - recall: 0.9823 - val_loss: 0.1107 - val_recall: 0.9790\n",
      "Epoch 549/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0755 - recall: 0.9816 - val_loss: 0.1283 - val_recall: 0.9738\n",
      "Epoch 550/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0746 - recall: 0.9819 - val_loss: 0.1177 - val_recall: 0.9767\n",
      "Epoch 551/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0663 - recall: 0.9850 - val_loss: 0.1127 - val_recall: 0.9778\n",
      "Epoch 552/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0695 - recall: 0.9825 - val_loss: 0.1100 - val_recall: 0.9796\n",
      "Epoch 553/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0722 - recall: 0.9821 - val_loss: 0.1154 - val_recall: 0.9778\n",
      "Epoch 554/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0682 - recall: 0.9823 - val_loss: 0.1127 - val_recall: 0.9790\n",
      "Epoch 555/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0767 - recall: 0.9816 - val_loss: 0.1294 - val_recall: 0.9749\n",
      "Epoch 556/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0682 - recall: 0.9823 - val_loss: 0.1117 - val_recall: 0.9784\n",
      "Epoch 557/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0697 - recall: 0.9820 - val_loss: 0.1174 - val_recall: 0.9790\n",
      "Epoch 558/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0680 - recall: 0.9833 - val_loss: 0.1065 - val_recall: 0.9790\n",
      "Epoch 559/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0666 - recall: 0.9842 - val_loss: 0.1179 - val_recall: 0.9784\n",
      "Epoch 560/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0675 - recall: 0.9843 - val_loss: 0.1122 - val_recall: 0.9778\n",
      "Epoch 561/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0677 - recall: 0.9843 - val_loss: 0.1099 - val_recall: 0.9808\n",
      "Epoch 562/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0700 - recall: 0.9835 - val_loss: 0.1305 - val_recall: 0.9749\n",
      "Epoch 563/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0739 - recall: 0.9802 - val_loss: 0.1254 - val_recall: 0.9761\n",
      "Epoch 564/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0686 - recall: 0.9827 - val_loss: 0.1194 - val_recall: 0.9773\n",
      "Epoch 565/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0674 - recall: 0.9841 - val_loss: 0.1231 - val_recall: 0.9767\n",
      "Epoch 566/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0678 - recall: 0.9835 - val_loss: 0.1259 - val_recall: 0.9755\n",
      "Epoch 567/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0735 - recall: 0.9826 - val_loss: 0.1432 - val_recall: 0.9708\n",
      "Epoch 568/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0765 - recall: 0.9785 - val_loss: 0.1087 - val_recall: 0.9790\n",
      "Epoch 569/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0717 - recall: 0.9850 - val_loss: 0.1134 - val_recall: 0.9796\n",
      "Epoch 570/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0682 - recall: 0.9840 - val_loss: 0.1261 - val_recall: 0.9755\n",
      "Epoch 571/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0676 - recall: 0.9848 - val_loss: 0.1186 - val_recall: 0.9784\n",
      "Epoch 572/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0666 - recall: 0.9830 - val_loss: 0.1068 - val_recall: 0.9802\n",
      "Epoch 573/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0671 - recall: 0.9844 - val_loss: 0.1085 - val_recall: 0.9796\n",
      "Epoch 574/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0752 - recall: 0.9810 - val_loss: 0.1081 - val_recall: 0.9808\n",
      "Epoch 575/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0644 - recall: 0.9859 - val_loss: 0.1162 - val_recall: 0.9784\n",
      "Epoch 576/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0650 - recall: 0.9852 - val_loss: 0.1156 - val_recall: 0.9796\n",
      "Epoch 577/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0709 - recall: 0.9822 - val_loss: 0.1221 - val_recall: 0.9755\n",
      "Epoch 578/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0688 - recall: 0.9832 - val_loss: 0.1272 - val_recall: 0.9743\n",
      "Epoch 579/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0698 - recall: 0.9812 - val_loss: 0.1135 - val_recall: 0.9778\n",
      "Epoch 580/600\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 0.0612 - recall: 0.9850 - val_loss: 0.1010 - val_recall: 0.9808\n",
      "Epoch 581/600\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 0.0754 - recall: 0.9811 - val_loss: 0.1123 - val_recall: 0.9778\n",
      "Epoch 582/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0712 - recall: 0.9819 - val_loss: 0.1160 - val_recall: 0.9784\n",
      "Epoch 583/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0682 - recall: 0.9813 - val_loss: 0.1129 - val_recall: 0.9778\n",
      "Epoch 584/600\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 0.0714 - recall: 0.9821 - val_loss: 0.1266 - val_recall: 0.9738\n",
      "Epoch 585/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0808 - recall: 0.9804 - val_loss: 0.1406 - val_recall: 0.9685\n",
      "Epoch 586/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0707 - recall: 0.9819 - val_loss: 0.1176 - val_recall: 0.9773\n",
      "Epoch 587/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0739 - recall: 0.9832 - val_loss: 0.1418 - val_recall: 0.9668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 588/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0642 - recall: 0.9841 - val_loss: 0.1214 - val_recall: 0.9761\n",
      "Epoch 589/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0677 - recall: 0.9842 - val_loss: 0.1068 - val_recall: 0.9796\n",
      "Epoch 590/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0679 - recall: 0.9847 - val_loss: 0.1122 - val_recall: 0.9784\n",
      "Epoch 591/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0680 - recall: 0.9842 - val_loss: 0.1114 - val_recall: 0.9790\n",
      "Epoch 592/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0684 - recall: 0.9840 - val_loss: 0.1223 - val_recall: 0.9773\n",
      "Epoch 593/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0721 - recall: 0.9833 - val_loss: 0.1173 - val_recall: 0.9778\n",
      "Epoch 594/600\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 0.0647 - recall: 0.9840 - val_loss: 0.1128 - val_recall: 0.9790\n",
      "Epoch 595/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0667 - recall: 0.9842 - val_loss: 0.1141 - val_recall: 0.9778\n",
      "Epoch 596/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0731 - recall: 0.9845 - val_loss: 0.1475 - val_recall: 0.9668\n",
      "Epoch 597/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0693 - recall: 0.9819 - val_loss: 0.1139 - val_recall: 0.9784\n",
      "Epoch 598/600\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 0.0648 - recall: 0.9848 - val_loss: 0.1152 - val_recall: 0.9773\n",
      "Epoch 599/600\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 0.0627 - recall: 0.9857 - val_loss: 0.1125 - val_recall: 0.9778\n",
      "Epoch 600/600\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.0675 - recall: 0.9826 - val_loss: 0.1146 - val_recall: 0.9790\n"
     ]
    }
   ],
   "source": [
    "window_dnn_reg = create_dnn(batchX_train, input_neurons, drop, output_neurons, True)\n",
    "\n",
    "window_dnn_reg_history = train(window_dnn_reg, batchX_train, batchY_train, \n",
    "                               batchX_val, batchY_val, criterion, optimizer, \n",
    "                               batch_size, num_epochs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "israeli-consciousness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(window_dnn_reg_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "complimentary-republic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.99458484 0.45283019]\n",
      "recall:    [0.98275862 0.72727273]\n",
      "fscore:    [0.98863636 0.55813953]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = plot_confusion_matrix(window_dnn_reg, batchX_val, np.argmax(batchY_val, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-marine",
   "metadata": {},
   "source": [
    "This is looking fairly good, but, ideally that bottom left square would be 0 (no false negatives). We can play with the sensitivity to try and get that square to 0, and then use the same threshold value for the test set. I am interested in doing this because I believe there are many instances where the danger level is 4, but the model is not able to distinguish it from a danger level of 3.\n",
    "\n",
    "In plain text, this strategy would be something like:\n",
    "> \"We care most about predicting a danger level 4 when it is a danger level 4, and less about over-predicting danger level 4's.\"\n",
    "\n",
    "So ideally, we would have a precision greater than 50% and a recall score as close to 100% as possible. **It might be worth looking at why the Keras recall score says 95+% on the validation set while the scikit-learn version gets only 78%.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "democratic-alarm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs = window_dnn_reg.predict(batchX_val)\n",
    "to_save = []\n",
    "for proba_pair in probs:\n",
    "    if(proba_pair[1] > 0.05):\n",
    "        to_save.append(1)\n",
    "    else: \n",
    "        to_save.append(0)\n",
    "\n",
    "#use seaborn's sns.heatmap() function for pretty plotting of confusion matrix\n",
    "cm = confusion_matrix(np.argmax(batchY_val, axis=1), to_save)\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d', cbar=False)\n",
    "\n",
    "#set x and y labels, as well as title\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('ROC Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-seafood",
   "metadata": {},
   "source": [
    "When we do this, we can limit the number of false negatives, but at the cost of predicting danger level 4 *much* more often. This makes me a little uncomfortable, because I don't want to over-optimize for the validation set and have it not generalize to the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-injury",
   "metadata": {},
   "source": [
    "Maybe we should use some sort of early stopping?\n",
    "- - https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "\n",
    "Or perhaps we could save the model each time we improve on recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "becoming-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall\n",
    "greater_than = np.where(np.array(window_dnn_reg_history.history['val_recall']) > 0.99)[0]\n",
    "for val in greater_than:\n",
    "    print('Recall:', window_dnn_reg_history.history['val_recall'][val])\n",
    "    \n",
    "# loss\n",
    "less_than = np.where(np.array(window_dnn_reg_history.history['val_loss']) < 0.05)[0] #it never reaches that low\n",
    "for val in less_than:\n",
    "    print('Loss:', window_dnn_reg_history.history['val_loss'][val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-administration",
   "metadata": {},
   "source": [
    "# Summary so far:\n",
    "- batching data works better than non-batching, although after trying several values for the window size, most perform the same\n",
    "- regularization definitely helps\n",
    "- DNNs perform the best overall, RNNs perform okay when only using the RegObs data\n",
    "- using both regobs and varsom data improves performance\n",
    "- class weights work well\n",
    "- classifying danger level 4 as an outlier works better than trying to classify danger levels 1-4\n",
    "- I am no longer optimizing for accuracy, only loss and recall\n",
    "- I am not sure of the effect of shuffling when using DNNs. Performance seems comparable.\n",
    "- 2 vs 3 layer DNNs perform similarly, although I received my best results so far with a 2 layer DNN. This might be worth looking at in hyperopt.\n",
    "\n",
    "I believe that I am reaching unreducible error in the validation set. I believe that there are some danger level 4's that look like 3's, and have never reached lower than 3 false negatives after training (and before playing with the prediction sensitivity). I think I have found a batch size, learning rate, epoch number, regularization parameter, and several other optimal hyperparameters, so it may be time to predict on the test set after running a bunch of models and saving the best one. I need to find a way to do this with the recall metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-image",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
